<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=EB Garamond:300,300italic,400,400italic,700,700italic|Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"shira0905.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="接下来实习要做的工作. NLP和DP没基础, 立此贴储备一些preliminary knowledge吧.">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT模型和文本纠错 [未完,待填坑]*">
<meta property="og:url" content="https://shira0905.github.io/2021/01/%E8%AF%BB%E6%87%82BERT%E6%A8%A1%E5%9E%8B/">
<meta property="og:site_name" content="圆头修行记">
<meta property="og:description" content="接下来实习要做的工作. NLP和DP没基础, 立此贴储备一些preliminary knowledge吧.">
<meta property="og:locale">
<meta property="og:image" content="https://shira0905.github.io/2021/01/%E8%AF%BB%E6%87%82BERT%E6%A8%A1%E5%9E%8B/image-20210215015407977.png">
<meta property="og:image" content="https://shira0905.github.io/2021/01/%E8%AF%BB%E6%87%82BERT%E6%A8%A1%E5%9E%8B/image-20210215020336485.png">
<meta property="article:published_time" content="2021-01-27T13:15:54.000Z">
<meta property="article:modified_time" content="2021-02-19T17:24:19.763Z">
<meta property="article:author" content="匠心圆头">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shira0905.github.io/2021/01/%E8%AF%BB%E6%87%82BERT%E6%A8%A1%E5%9E%8B/image-20210215015407977.png">

<link rel="canonical" href="https://shira0905.github.io/2021/01/%E8%AF%BB%E6%87%82BERT%E6%A8%A1%E5%9E%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>BERT模型和文本纠错 [未完,待填坑]* | 圆头修行记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">圆头修行记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">不自见，故明；不自是，故彰；<br>不自伐，故有功；不自矜，故长。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://shira0905.github.io/2021/01/%E8%AF%BB%E6%87%82BERT%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zsy.jpg">
      <meta itemprop="name" content="匠心圆头">
      <meta itemprop="description" content="致虚极，守静笃。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圆头修行记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          BERT模型和文本纠错 [未完,待填坑]*
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-27 21:15:54" itemprop="dateCreated datePublished" datetime="2021-01-27T21:15:54+08:00">2021-01-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-20 01:24:19" itemprop="dateModified" datetime="2021-02-20T01:24:19+08:00">2021-02-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index"><span itemprop="name">Research</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/base/" itemprop="url" rel="index"><span itemprop="name">base</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>接下来实习要做的工作. NLP和DP没基础, 立此贴储备一些preliminary knowledge吧.</p>
<a id="more"></a>
<h1 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h1><h3 id="什么是预训练"><a href="#什么是预训练" class="headerlink" title="什么是预训练"></a>什么是预训练</h3><ul>
<li>你需要搭建一个网络模型来完成一个特定的图像分类的任务。首先，你需要随机初始化参数，然后开始训练网络，不断调整直到网络的损失越来越小。在训练的过程中，一开始初始化的参数会不断变化。当你觉得结果很满意的时候，你就可以将训练模型的参数保存下来，以便训练好的模型可以在下次执行类似任务时获得较好的结果。这个过程就是 pre-training。</li>
</ul>
<p>深度网络存在问题:</p>
<ol>
<li>网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。</li>
<li>多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；</li>
<li>梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。</li>
</ol>
<p>解决方法:</p>
<p>逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。</p>
<p>经过预训练最终能得到比较好的局部最优解。</p>
<h4 id="为什么我们要做预训练模型？"><a href="#为什么我们要做预训练模型？" class="headerlink" title="为什么我们要做预训练模型？"></a>为什么我们要做预训练模型？</h4><ul>
<li>首先，预训练模型是一种迁移学习的应用，利用几乎无限的文本，学习输入句子的每一个成员的上下文相关的表示，它隐式地学习到了通用的语法语义知识。</li>
<li>第二，它可以将从开放领域学到的知识迁移到下游任务，以改善低资源任务，对低资源语言处理也非常有利。</li>
<li>第三，预训练模型在几乎所有 NLP 任务中都取得了目前最佳的成果。最后，这个预训练模型+微调机制具备很好的可扩展性，在支持一个新任务时，只需要利用该任务的标注数据进行微调即可，一般工程师就可以实现。</li>
</ul>
<h4 id="预训练模型的三个关键技术"><a href="#预训练模型的三个关键技术" class="headerlink" title="预训练模型的三个关键技术"></a>预训练模型的三个关键技术</h4><h5 id="1-Transformer"><a href="#1-Transformer" class="headerlink" title="1 Transformer"></a>1 Transformer</h5><p>是预训练语言模型的核心网络. </p>
<p>输入是一句话或者一个段落.  每个词转化为词向量, 同时加上每一个词的位置向量 —&gt; 体现词在序列的位置.</p>
<p>将这些词向量输入到多层的Transformer 网络中, 通过自注意力机制来学习词与词之间的关系, 编码其上下文信息.</p>
<p>再通过一个前馈网络经过非线性变化，输出综合了上下文特征的各个词的向量表示. </p>
<p>【所以这个网络主要的功能是把词转化成合理的向量】</p>
<p>每一层 Transformer 网络主要由 Multi-head self-attention 层（多头自注意力机制）和前馈网络层两个子层构成。Multi-head self-attention 会并行地执行多个不同参数的 self-attention，并将各个 self-attention 的结果拼接作为后续网络的输入，self-attention 机制会在后面中做详细介绍. </p>
<p>此后，我们得到了蕴含当前上下文信息的各个词的表示，然后网络会将其输入到前馈网络层以计算非线性层次的特征。</p>
<p>在每一层 Transformer 网络中，会将残差连接（residual connection）把自注意力机制前或者前馈神经网络之前的向量引入进来，以增强自注意力机制或者前馈网络的输出结果向量。并且还做一个 layer normalization，也就是通过归一化把同层的各个节点的多维向量映射到一个区间里面，这样各层节点的向量在一个区间里面。这两个操作加入在每个子层后，可更加平滑地训练深层次网络。</p>
<p>Transformer 可以用于编码，也可以用于解码。所谓<strong>解码</strong>就是根据一个句子的输入得到一个预想的结果，比如机器翻译（输入源语言句子，输出目标语言句子），或者阅读理解（输入文档和问题，输出答案）。解码时，已经解码出来的词要做一个自注意力机制，之后和编码得到的隐状态的序列再做一个注意力机制。这样可以做 N 层，然后通过一个线性层映射到词表的大小的一个向量。每<strong>个向量代表一个词表词的输出可能性</strong>，经过一个softmax 层得到每个词的输出概率。[那这个向量纬度等于词表的size是吗]【那编码是什么？】</p>
<p>接下来介绍一下 self-attention 机制: Q, K, V 三个向量, 加权什么的没看懂.在得到第一个词的上下文表示后，给定第二个词的 query 向量，我们会重复之前的操作，计算当前 query 向量同各个词 key 向量的得分，对这些得分做 softmax 归一化处理，并将这些得分同其对应的 value 向量做加权和，以得到其编码上下文信息的表示。</p>
<h5 id="2-自监督学习"><a href="#2-自监督学习" class="headerlink" title="2 自监督学习"></a>2 自监督学习</h5><p>自回归语言模型: 用前面的词序列  预测下个词的出现概率.</p>
<p>自动编码器: 旨在对损坏的输入句子, 比如遮掩了句子某个词、或者打乱了词序等，重建原始数据。通过这些自监督学习手段来学习单词的上下文相关表示。</p>
<h5 id="3-微调"><a href="#3-微调" class="headerlink" title="3 微调"></a>3 微调</h5><p>利用其标注样本 对预训练网络的参数进行调整.</p>
<p>我们使用基于 BERT（一种流行的预训练模型）为例来判断两个句子是否语义相同。</p>
<p>输入是两个句子，经过 BERT 得到每个句子的对应编码表示，我们可以简单地用预训练模型的第一个隐节点预测分类标记判断两个句子是同义句子的概率.        同时需要额外加一个线性层和 softmax 计算得到分类标签的分布。预测损失可以反传给 BERT 再对网络进行微调。当然也可以针对具体任务设计一个新网络，把预训练的结果作为其输入。</p>
<h5 id="预训练模型发展趋势"><a href="#预训练模型发展趋势" class="headerlink" title="预训练模型发展趋势"></a>预训练模型发展趋势</h5><ol>
<li>模型越来越大 —&gt; 导致模型的参数越来越大 —&gt; 其能力也会越来越强，但是训练代价确实非常大</li>
<li>预训练方法也在不断增加: 从自回归 LM，到自动编码的各种方法，以及各种多任务训练等</li>
<li>还有从语言、多语言到多模态不断演进</li>
<li>最后就是模型压缩，使之能在实际应用中经济的使用，比如在手机端。这就涉及到知识蒸馏和 teacher-student models，把大模型作为 teacher，让一个小模型作为 student 来学习，接近大模型的能力，但是模型的参数减少很多。</li>
</ol>
<h3 id="什么是模型微调fine-tuning"><a href="#什么是模型微调fine-tuning" class="headerlink" title="什么是模型微调fine tuning"></a>什么是模型微调fine tuning</h3><p>用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning).</p>
<p><strong>模型的微调举例说明：</strong></p>
<p> 我们知道，CNN 在图像识别这一领域取得了巨大的进步。如果想将 CNN 应用到我们自己的数据集上，这时通常就会面临一个问题：通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。如果只利用手头上这点数据，即使我们利用非常好的网络结构，也达不到很高的 performance。这时候，fine-tuning 的思想就可以很好解决我们的问题：我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。</p>
<p>所以，<strong>预训练</strong> 就是指预先训练的一个模型或者指预先训练模型的过程；<strong>微调</strong> 就是指将预训练过的模型作用于自己的数据集，并使参数适应自己数据集的过程</p>
<h3 id="微调时候网络参数是否更新？"><a href="#微调时候网络参数是否更新？" class="headerlink" title="微调时候网络参数是否更新？"></a>微调时候网络参数是否更新？</h3><p>答案：会更新。</p>
<ol>
<li>finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。 </li>
<li>直接训练是按照网络定义指定的方式初始化。</li>
<li>finetune是用你已经有的参数文件来初始化。</li>
</ol>
<h3 id="fine-tuning-模型的三种状态"><a href="#fine-tuning-模型的三种状态" class="headerlink" title="fine-tuning 模型的三种状态"></a>fine-tuning 模型的三种状态</h3><ol>
<li>状态一：只预测，不训练。 特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</li>
<li>状态二：训练，但只训练最后分类层。 特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</li>
<li>状态三：完全训练，分类层+之前卷积层都训练 特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。</li>
</ol>
<h3 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h3><p>预训练模型的训练方法可使用自监督学习技术（如自回归的语言模型和自编码技术</p>
<p>预训练通过自监督学习从大规模数据中获得与具体任务无关的预训练模型。</p>
<h1 id="Bert使用方法"><a href="#Bert使用方法" class="headerlink" title="Bert使用方法"></a>Bert使用方法</h1><p>下载bert</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;bert.git</span><br></pre></td></tr></table></figure>
<p>下载bert预训练模型</p>
<p>Google提供了多种预训练好的bert模型，从下方链接下载 Google 发布的<a href="https://link.zhihu.com/?target=https%3A//github.com/google-research/bert%23pre-trained-models">预训练模型</a>，解压到某个路径下，比如： /tmp/english_L-12_H-768_A-12/</p>
<p>你可以使用包括 BERT-Base, Multilingual 和 BERT-Base, Chinese 在内的任意模型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python app.py -model_dir &#x2F;tmp&#x2F;english_L-12_H-768_A-12&#x2F; -num_worker&#x3D;4</span><br></pre></td></tr></table></figure>
<p>3.（可选项）安装bert-as-service，这是一个可以利用bert模型将句子映射到固定长度向量的服务。</p>
<p>BERT 模型的使用主要有两种用途：<br>一、当作<strong>文本特征提取</strong>的工具，类似Word2vec模型一样<br>二、作为一个可训练的层，后面可接入客制化的网络，做<strong>迁移学习</strong></p>
<p>作为特征提取的工具，有一种简单的使用方式：<a href="https://link.zhihu.com/?target=https%3A//github.com/hanxiao/bert-as-service">bert-as-service</a><br>首先，需要安装 server 和 client 两个包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install bert-serving-server # 服务端</span><br><span class="line">pip install bert-serving-client # 客户端</span><br></pre></td></tr></table></figure>
<p>环境要求： Python &gt;= 3.5 Tensorflow &gt;= 1.10<br>然后，下载 BERT 预训练模型，可以点击上述链接下载，比如我们下载中文版本 BERT 模型 <strong><a href="https://link.zhihu.com/?target=https%3A//storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip">BERT-Base, Chinese</a></strong> 。下载完成后，解压到本地某个目录下。<br>例如：/tmp/chinese_L-12_H-768_A-12/<br>然后，打开终端，输入以下命令启动服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert-serving-start -model_dir ~&#x2F;Desktop&#x2F;chinese_L-12_H-768_A-12&#x2F;  -num_worker&#x3D;2 </span><br></pre></td></tr></table></figure>
<p>其中，参数 model_dir 设置为解压得到的BERT预训练模型路径，num_worker为进程数。需要说明的是，num_worker 必须小于CPU的核心数或GPU设备数。</p>
<p>除了 bert-as-service 这种使用方式外，当然也可以利用Tensorflow、Keras等深度学习框架<strong>重建 BERT 预训练模型</strong>，然后利用重建的BERT模型去获取文本的向量表示。</p>
<h1 id="知乎-XLNet-运行机制及和Bert的异同比较"><a href="#知乎-XLNet-运行机制及和Bert的异同比较" class="headerlink" title="知乎: XLNet:运行机制及和Bert的异同比较"></a>知乎: XLNet:运行机制及和Bert的异同比较</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70257427">https://zhuanlan.zhihu.com/p/70257427</a></p>
<h2 id="行情"><a href="#行情" class="headerlink" title="行情"></a>行情</h2><p>在某些场景下，确实XLNet相对Bert有很大幅度的提升。就像我们之前说的，感觉<strong>Bert打开两阶段模式</strong>的魔法盒开关后，在这条路上，会有越来越多的同行者，而XLNet就是其中比较引人注目的一位. 未来两年，在<strong>两阶段新模式（预训练+Finetuning）</strong>下，应该会有更多的好工作涌现出来。根本原因在于：这个模式的潜力还没有被充分挖掘，貌似还有很大的提升空间。</p>
<p>现在其实是进入NLP领域非常好的时机。原因有两个，</p>
<ul>
<li>一个是NLP正面临一个技术栈大的改朝换代的时刻，有很多空白等着你去填补，容易出成绩；</li>
<li>另外一点，貌似<strong>Bert+Transformer有统一NLP各个应用领域的趋向</strong>，这意味着此时进入NLP领域，具备学习成本非常低的好处，和之前相比，投入产出比非常合算</li>
</ul>
<h2 id="XLNet引入了自回归语言模型以及自编码语言模型的提法"><a href="#XLNet引入了自回归语言模型以及自编码语言模型的提法" class="headerlink" title="XLNet引入了自回归语言模型以及自编码语言模型的提法"></a>XLNet引入了自回归语言模型以及自编码语言模型的提法</h2><h4 id="自回归语言模型（Autoregressive-LM）"><a href="#自回归语言模型（Autoregressive-LM）" class="headerlink" title="自回归语言模型（Autoregressive LM）"></a>自回归语言模型（Autoregressive LM）</h4><p>在ELMO（Embeddings from Language Models）／BERT出来之前，大家<strong>通常讲的语言模型</strong>其实是根据上文内容预测下一个可能跟随的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词，这种类型的LM被称为<strong>自回归语言模型</strong>。</p>
<p>缺点: 只能利用上文或者下文的信息，不能同时利用上文和下文的信息，当然，貌似ELMO这种双向都做，然后拼接看上去能够解决这个问题，因为融合模式过于简单，所以效果其实并不是太好。</p>
<p>优点: 其实跟下游NLP任务有关，比如生成类NLP任务，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。(而Bert这种DAE模式，在生成类NLP任务中，就面临训练过程和应用过程不一致的问题，导致生成类的NLP任务到目前为止都做不太好。)</p>
<h4 id="自编码语言模型（Autoencoder-LM）"><a href="#自编码语言模型（Autoencoder-LM）" class="headerlink" title="自编码语言模型（Autoencoder LM）"></a>自编码语言模型（Autoencoder LM）</h4><p>相比而言，Bert通过在输入X中随机Mask掉一部分单词，然后<strong>预训练过程的主要任务之一</strong>是根<strong>据上下文单词</strong>来<strong>预测这些被Mask掉的</strong>单词.</p>
<p>如果你对Denoising Autoencoder比较熟悉的话，会看出，这确实是典型的DAE(去噪声自编码器)的思路。那些被Mask掉的单词就是在输入侧加入的所谓噪音。类似Bert这种预训练模式，被称为DAE LM。</p>
<p>优点: 它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，这是好处。</p>
<p>缺点: 主要在输入侧引入[Mask]标记，<strong>导致预训练阶段和Fine-tuning阶段不一致的问题</strong>，因为Fine-tuning阶段是看不到[Mask]标记的。</p>
<h4 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h4><p>出发点就是：能否融合自回归LM和DAE LM两者的优点。就是说如果站在自回归LM的角度，如何引入和双向语言模型等价的效果；如果站在DAE LM的角度看，它本身是融入双向语言模型的，如何抛掉表面的那个[Mask]标记，让预训练和Fine-tuning保持一致</p>
<p>阅读理解任务: GPT 2.0的作者不信这个邪，坚持沿用GPT 1.0 单向语言模型的旧瓶，装进去了更高质量更大规模预训练数据的新酒，如果想改善预训练语言模型，走这条扩充预序列模型训练数据的路子，是个多快好但是不省钱的方向。这也进一步说明了，预训练LM这条路，还远远没有走完，还有很大的提升空间，比如最简单的提升方法就是加大数据规模，提升数据质量。</p>
<p>尽管可以类似ELMO两个都做，然后再拼接的方式。但是跟Bert比，效果明显不足够好（这里面有RNN弱于Transformer的因素，也有双向语言模型怎么做的因素）</p>
<p>XLNet仍然遵循两阶段的过程，第一个阶段是语言模型预训练阶段；第二阶段是任务数据Fine-tuning阶段。</p>
<p>它主要希望改动第一个阶段，就是说不像Bert那种带Mask符号的Denoising-autoencoder的模式, 而是采用自回归LM的模式.     就是说，看上去输入句子X仍然是自左向右的输入，看到Ti单词的上文Context_before，来预测Ti这个单词。但是又希望在Context_before里，不仅仅看到上文单词，也能看到Ti单词后面的下文Context_after里的下文单词，这样的话，Bert里面预训练阶段引入的Mask符号就不需要了，于是在预训练阶段，看上去是个标准的从左向右过程，Fine-tuning当然也是这个过程，于是两个环节就统一起来。当然，这是目标。剩下是怎么做到这一点的问题。怎么能够在单词Ti的上文中Contenxt_before中揉入下文Context_after的内容呢？</p>
<blockquote>
<p>XLNet是这么做的，在预训练阶段，引入Permutation Language Model的训练目标。什么意思呢？就是说，比如包含单词Ti的当前输入的句子X，由顺序的几个单词构成，比如x1,x2,x3,x4四个单词顺序构成。我们假设，其中，要预测的单词Ti是x3，位置在Position 3，要想让它能够在上文Context_before中，也就是Position 1或者Position 2的位置看到Position 4的单词x4。可以这么做：假设我们固定住x3所在位置，就是它仍然在Position 3，之后随机排列组合句子中的4个单词，在随机排列组合后的各种可能里，再选择一部分作为模型预训练的输入X。比如随机排列组合后，抽取出x4,x2，x3,x1这一个排列组合作为模型的输入X。于是，x3就能同时看到上文x2，以及下文x4的内容了。这就是XLNet的基本思想，所以说，看了这个就可以理解上面讲的它的初衷了吧：看上去仍然是个自回归的从左到右的语言模型，但是其实通过对句子中单词排列组合，把一部分Ti下文的单词排到Ti的上文位置中，于是，就看到了上文和下文，但是形式上看上去仍然是从左到右在预测后一个单词。</p>
</blockquote>
<h1 id="知乎-Soft-Masked-BERT：文本纠错与BERT的最新结合"><a href="#知乎-Soft-Masked-BERT：文本纠错与BERT的最新结合" class="headerlink" title="知乎: Soft-Masked BERT：文本纠错与BERT的最新结合"></a>知乎: Soft-Masked BERT：文本纠错与BERT的最新结合</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/144995580">https://zhuanlan.zhihu.com/p/144995580</a></p>
<p><strong>文本纠错</strong>，是自然语言处理领域检测一段文字是否存在错别字、以及将错别字纠正过来的技术，一般用于文本预处理阶段，同时能显著缓解智能客服等场景下语音识别（ASR）不准确的问题。</p>
<h3 id="一-文本纠错示例与难点"><a href="#一-文本纠错示例与难点" class="headerlink" title="一.文本纠错示例与难点"></a>一.文本纠错示例与难点</h3><p>生活中常见的文本错误可以分为（1）字形相似引起的错误（2）拼音相似引起的错误 两大类；如：“咳数”-&gt;“咳嗽”；“哈蜜”-&gt;“哈密”。错别字往往来自于如下的“相似字典”。</p>
<p>现有的NLP技术已经能解决多数文本拼写错误。剩余的<strong>纠错难点</strong>主要在于，部分文本拼写错误需要<strong>常识背景（world-knowledge）</strong>才能识别。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Wrong: &quot;我想去埃及金子塔旅游。&quot;</span><br><span class="line">Right: &quot;我想去埃及金字塔旅游。&quot;</span><br></pre></td></tr></table></figure>
<p>同时，一些错误需要模型像人一样具备<strong>一定的推理和分析能力</strong>才能识破。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Wrong: &quot;他的求胜欲很强，为了越狱在挖洞。&quot;</span><br><span class="line">Right: &quot;他的求生欲很强，为了越狱在挖洞。&quot;</span><br></pre></td></tr></table></figure>
<p>“求胜欲”和“求生欲”在自然语言中都是正确的，但是结合上下文语境来分析，显然后者更为合适。</p>
<p>最后，<strong>文本纠错技术对于误判率（将正确的词“纠正”成错误的）有严格的要求，一般要求低于0.5%</strong>。如果纠错方法的误判率很高，会对系统和用户体验有很差的负面效果。</p>
<h3 id="二-文本纠错常用技术"><a href="#二-文本纠错常用技术" class="headerlink" title="二.文本纠错常用技术"></a>二.文本纠错常用技术</h3><p>常用的方法可以归纳为错别字词典、编辑距离、语言模型等。</p>
<p>构建错别字词典人工成本较高，适用于错别字有限的部分垂直领域；编辑距离采用类似字符串模糊匹配的方法，通过对照正确样本可以纠正部分常见错别字和语病，但是通用性不足。</p>
<p>所以，现阶段学术界和工业界研究的重点一般都是基于语言模型的纠错技术。2018年之前，语言模型的方法可以分为<strong>传统的n-gram LM和DNN LM</strong>，可以以字或词为纠错粒度。其中“字粒度”的语义信息相对较弱，因此误判率会高于“词粒度”的纠错；“词粒度”则较依赖于分词模型的准确率。【n-gram LM和DNN LM 是什么】</p>
<p>2018年之后，预训练语言模型开始流行，研究人员很快把BERT类的模型迁移到了文本纠错中，并取得了新的最优效果。【与训练语言模型是什么?  BERT类? 模型迁移?】</p>
<h3 id="三、将BERT应用于文本纠错"><a href="#三、将BERT应用于文本纠错" class="headerlink" title="三、将BERT应用于文本纠错"></a>三、将BERT应用于文本纠错</h3><p>BERT与以往深度学习模型的主要区别在于：预训练阶段使用了两个任务，特征抽取使用12层双向Transformer，更大的训练语料和机器</p>
<ul>
<li><p>“掩码语言模型”MLM</p>
<p>其中，MLM任务使得模型并不知道输入位置的词汇是否为正确的词汇（10%概率），这就迫使模型更多地依赖于上下文信息去预测词汇，赋予了模型一定的纠错能力。</p>
<p>一种简单的使用方式为，依次将文本s中的每一个字c做mask掩码，依赖c的上下文来预测c位置最合适的字（假设词表大小为20000，相当于在句子中的每一个位置做了一个“20000分类”）。设置一个容错阈值k=5，如果原先的字c出现在预测结果的top5中，就认为该位置不是错别字，否则是错别字。</p>
<p>当然这种方法过于粗暴，很可能造成高误判率。作为优化，我们可以采用预训练的方式对BERT进行微调，显著改进纠错效果。纠错的领域最好和微调领域相同（如果需要在新闻类文章中纠错，可以使用“人民日报语料”对模型微调）。</p>
</li>
<li><p>和“判断s1是否为s2下一句”NSP</p>
</li>
</ul>
<p>除了上述方法之外，还有很多 trick 可以提升 BERT 的效果与速度。这里推荐大家读一下 bienlearn 上的 BERT 专栏。</p>
<h3 id="四、文本纠错最优模型：Soft-Masked-BERT"><a href="#四、文本纠错最优模型：Soft-Masked-BERT" class="headerlink" title="四、文本纠错最优模型：Soft-Masked BERT"></a>四、文本纠错最优模型：Soft-Masked BERT</h3><p>为了弥补baseline方法的不足，最大限度发挥BERT功效</p>
<p>论文首次提出了Soft-Masked BERT模型，主要创新点在于：</p>
<p>（1）将文本纠错划分为检测网络（Detection）和纠正网络（Correction）两部分，纠正网络的输入来自于检测网络输出。</p>
<p>（2）以检测网络的输出作为权重，将 masking-embedding以“soft方式”添加到各个字符特征上，即“Soft-Masked”。</p>
<h1 id="B站三节课"><a href="#B站三节课" class="headerlink" title="B站三节课"></a>B站三节课</h1><p>做NLP用 pytorch 比 tensorflow 更为方便.</p>
<p>transformer </p>
<p>主要是在变换</p>
<p>不用bert paper 里面那么多的参数效果也可以不错.</p>
<p>主要是要搞清楚在每一步矩阵的形状, dimension要明明白白</p>
<h1 id="CSDN论文解读"><a href="#CSDN论文解读" class="headerlink" title="CSDN论文解读"></a>CSDN论文解读</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35128926/article/details/106770581">https://blog.csdn.net/qq_35128926/article/details/106770581</a></p>
<h1 id="Pycorrector"><a href="#Pycorrector" class="headerlink" title="Pycorrector"></a>Pycorrector</h1><p> <a target="_blank" rel="noopener" href="https://github.com/shibing624/pycorrector">pycorrector</a></p>
<p>中文文本纠错任务，常见错误类型包括：</p>
<ul>
<li>谐音字词，如 配副眼睛-配副眼镜</li>
<li>混淆音字词，如 流浪织女-牛郎织女</li>
<li>字词顺序颠倒，如 伍迪艾伦-艾伦伍迪</li>
<li>字词补全，如 爱有天意-假如爱有天意</li>
<li>形似字错误，如 高梁-高粱</li>
<li>中文拼音全拼，如 xingfu-幸福</li>
<li>中文拼音缩写，如 sz-深圳</li>
<li>语法错误，如 想象难以-难以想象</li>
</ul>
<p>当然，针对不同业务场景，这些问题并不一定全部存在，比如输入法中需要处理前四种，搜索引擎需要处理所有类型，语音识别后文本纠错只需要处理前两种， 其中’形似字错误’主要针对五笔或者笔画手写输入等。本项目重点解决其中的谐音、混淆音、形似字错误、中文拼音全拼、语法错误带来的纠错任务。</p>
<h3 id="规则的解决思路"><a href="#规则的解决思路" class="headerlink" title="规则的解决思路"></a>规则的解决思路</h3><p>中文纠错分为两步走，</p>
<ol>
<li><p>第一步是错误检测</p>
<p>错误检测部分先通过结巴中文分词器切词，由于句子中含有错别字，所以切词结果往往会有切分错误的情况，这样从字粒度和词粒度两方面检测错误， 整合这两种粒度的疑似错误结果，形成疑似错误位置候选集</p>
</li>
<li><p>第二步是错误纠正；</p>
<p>错误纠正部分，是遍历所有的疑似错误位置，并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，对所有候选集结果比较并排序，得到最优纠正词。</p>
</li>
</ol>
<p>【字粒度和词粒度两方面检测错误】</p>
<p>【通过语言模型计算句子困惑度】</p>
<h3 id="深度模型的解决思路"><a href="#深度模型的解决思路" class="headerlink" title="深度模型的解决思路"></a>深度模型的解决思路</h3><ol>
<li>端到端的深度模型可以避免人工提取特征，减少人工工作量，RNN序列模型对文本任务拟合能力强，rnn_attention在英文文本纠错比赛中取得第一名成绩，证明应用效果不错；</li>
<li>CRF会计算全局最优输出节点的条件概率，对句子中特定错误类型的检测，会根据整句话判定该错误，阿里参赛2016中文语法纠错任务并取得第一名，证明应用效果不错；</li>
<li>Seq2Seq模型是使用Encoder-Decoder结构解决序列转换问题，目前在序列转换任务中（如机器翻译、对话生成、文本摘要、图像描述）使用最广泛、效果最好的模型之一；</li>
<li>BERT/ELECTRA/ERNIE/MacBERT等预训练模型强大的语言表征能力，对NLP届带来翻天覆地的改变，海量的训练数据拟合的语言模型效果无与伦比，基于其MASK掩码的特征，可以简单改造预训练模型用于纠错，加上fine-tune，效果轻松达到最优。</li>
</ol>
<p>【什么是端到端?  什么是RNN序列模型和rnn_attention?  CRF是什么?  Seq2Seq模型是什么? 编码解码是什么? 序列转换是什么? 】</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul>
<li>kenlm：kenlm统计语言模型工具，规则方法，语言模型纠错，利用混淆集，扩展性强</li>
<li>deep_context模型：参考Stanford University的nlc模型，该模型是参加2014英文文本纠错比赛并取得第一名的方法</li>
<li>Seq2Seq模型：在Seq2Seq模型加上attention机制，对于长文本效果更好，模型更容易收敛，但容易过拟合</li>
<li>ConvSeq2Seq模型：基于Facebook出品的fairseq，北京语言大学团队改进ConvS2S模型用于中文纠错，在NLPCC-2018的中文语法纠错比赛中，是唯一使用单模型并取得第三名的成绩</li>
<li>transformer模型：全attention的结构代替了lstm用于解决sequence to sequence问题，语义特征提取效果更好</li>
<li>BERT模型：中文fine-tuned模型，使用MASK特征纠正错字</li>
<li>ELECTRA模型：斯坦福和谷歌联合提出的一种更具效率的预训练模型，学习文本上下文表示优于同等计算资源的BERT和XLNet</li>
<li>ERNIE模型：百度公司提出的基于知识增强的语义表示模型，有可适配中文的强大语义表征能力。在情感分析、文本匹配、自然语言推理、词法分析、阅读理解、智能问答等16个公开数据集上超越世界领先技术</li>
<li>MacBERT模型：来自哈工大SCIR实验室2020年的工作，改进了BERT模型的训练方法，使用全词掩蔽和N-Gram掩蔽策略适配中文表达，和通过用其相似的单词来掩盖单词，从而缩小训练前和微调阶段之间的差距</li>
</ul>
<p>【gosh！什么都不会 唔！】</p>
<h3 id="错误检测"><a href="#错误检测" class="headerlink" title="错误检测"></a>错误检测</h3><ul>
<li>字力度: 语言模型困惑度（ppl）检测某个字的似然概率值低于句子文本平均值, 则判定是错别字的概率大</li>
<li>词力度: 切词后不再词典中的词疑似错别字的概率较大</li>
</ul>
<h3 id="错误纠正"><a href="#错误纠正" class="headerlink" title="错误纠正"></a>错误纠正</h3><ul>
<li>通过错误检测定位疑似错误之后, 取所有疑似错字的音似or形似候选词</li>
<li>使用候选词替换, 【基于语言模型得到类似翻译模型的？？】得到候选排序结果, 得到最优就证词</li>
</ul>
<h3 id="测试规则方法"><a href="#测试规则方法" class="headerlink" title="测试规则方法?"></a>测试规则方法?</h3><p>全自动安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pycorrector</span><br></pre></td></tr></table></figure>
<p>【第一次运行的时候很慢，我猜可能是第一次使用会下载语言模型】</p>
<h4 id="文本纠错"><a href="#文本纠错" class="headerlink" title="文本纠错:"></a>文本纠错:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycorrector</span><br><span class="line">corrected_sent, detail = pycorrector.correct(<span class="string">&#x27;少先队员因该为老人让坐&#x27;</span>)</span><br><span class="line">print(corrected_sent, detail)</span><br><span class="line"><span class="comment"># 少先队员应该为老人让座 [[(&#x27;因该&#x27;, &#x27;应该&#x27;, 4, 6)], [(&#x27;坐&#x27;, &#x27;座&#x27;, 10, 11)]]</span></span><br></pre></td></tr></table></figure>
<p>规则方法默认会从路径<code>~/.pycorrector/datasets/zh_giga.no_cna_cmn.prune01244.klm</code>加载<strong>kenlm语言模型文件</strong>，如果检测没有该文件，则程序会自动联网下载。当然也可以手动下载<a target="_blank" rel="noopener" href="https://deepspeech.bj.bcebos.com/zh_lm/zh_giga.no_cna_cmn.prune01244.klm">模型文件(2.8G)</a>并放置于该位置。</p>
<h4 id="错误检测-1"><a href="#错误检测-1" class="headerlink" title="错误检测:"></a>错误检测:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycorrector</span><br><span class="line">idx_errors = pycorrector.detect(<span class="string">&#x27;少先队员因该为老人让坐&#x27;</span>)</span><br><span class="line">print(idx_errors)</span><br><span class="line"><span class="comment"># 返回类型是list, [error_word, begin_pos, end_pos, error_type]，pos索引位置以0开始</span></span><br><span class="line"><span class="comment"># [[&#x27;因该&#x27;, 4, 6, &#x27;word&#x27;], [&#x27;坐&#x27;, 10, 11, &#x27;char&#x27;]]</span></span><br></pre></td></tr></table></figure>
<h4 id="关闭字粒度纠错"><a href="#关闭字粒度纠错" class="headerlink" title="关闭字粒度纠错:"></a>关闭字粒度纠错:</h4><p>默认字粒度、词粒度的纠错都打开，一般情况下单字错误发生较少，而且字粒度纠错准确率较低。关闭字粒度纠错，这样可以提高纠错准确率，提高纠错速度。</p>
<blockquote>
<p>默认<code>enable_char_error</code>方法的<code>enable</code>参数为<code>True</code>，即打开错字纠正，这种方式可以召回字粒度错误，但是整体准确率会低. </p>
<p>如果追求准确率而不追求召回率的话，建议将<code>enable</code>设为<code>False</code>，仅使用错词纠正。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycorrector</span><br><span class="line"></span><br><span class="line">error_sentence_1 = <span class="string">&#x27;我的喉咙发炎了要买点阿莫细林吃&#x27;</span></span><br><span class="line">correct_sent = pycorrector.correct(error_sentence_1)</span><br><span class="line">print(correct_sent)</span><br><span class="line"><span class="comment"># &#x27;我的喉咙发炎了要买点阿莫西林吉&#x27;, [[&#x27;细林&#x27;, &#x27;西林&#x27;, 12, 14], [&#x27;吃&#x27;, &#x27;吉&#x27;, 14, 15]]</span></span><br><span class="line"></span><br><span class="line">error_sentence_1 = <span class="string">&#x27;我的喉咙发炎了要买点阿莫细林吃&#x27;</span></span><br><span class="line">pycorrector.enable_char_error(enable=<span class="literal">False</span>) <span class="comment"># 新加</span></span><br><span class="line">correct_sent = pycorrector.correct(error_sentence_1)</span><br><span class="line">print(correct_sent)</span><br><span class="line"><span class="comment"># &#x27;我的喉咙发炎了要买点阿莫西林吃&#x27;, [[&#x27;细林&#x27;, &#x27;西林&#x27;, 12, 14]]</span></span><br></pre></td></tr></table></figure>
<h4 id="加载自定义混淆集"><a href="#加载自定义混淆集" class="headerlink" title="加载自定义混淆集"></a>加载自定义混淆集</h4><p>什么是混淆集? 通过加载自定义混淆集，支持用户纠正已知的错误，包括两方面功能：1）错误补召回；2）误杀加白。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycorrector</span><br><span class="line"></span><br><span class="line">pycorrector.set_log_level(<span class="string">&#x27;INFO&#x27;</span>)</span><br><span class="line">error_sentences = [</span><br><span class="line">    <span class="string">&#x27;买iphonex，要多少钱&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;共同实际控制人萧华、霍荣铨、张旗康&#x27;</span>,</span><br><span class="line">]</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> error_sentences:</span><br><span class="line">    print(pycorrector.correct(line))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;*&#x27;</span> * <span class="number">53</span>)</span><br><span class="line">pycorrector.set_custom_confusion_dict(path=<span class="string">&#x27;./my_custom_confusion.txt&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> error_sentences:</span><br><span class="line">    print(pycorrector.correct(line))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># (&#x27;买iphonex，要多少钱&#x27;, [])   # &quot;iphonex&quot;漏召，应该是&quot;iphoneX&quot;</span></span><br><span class="line"><span class="comment"># (&#x27;共同实际控制人萧华、霍荣铨、张启康&#x27;, [[&#x27;张旗康&#x27;, &#x27;张启康&#x27;, 14, 17]]) # &quot;张启康&quot;误杀，应该不用纠</span></span><br><span class="line"><span class="comment"># *****************************************************</span></span><br><span class="line"><span class="comment"># (&#x27;买iphonex，要多少钱&#x27;, [[&#x27;iphonex&#x27;, &#x27;iphoneX&#x27;, 1, 8]])</span></span><br><span class="line"><span class="comment"># (&#x27;共同实际控制人萧华、霍荣铨、张旗康&#x27;, [])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中./my_custom_confusion.txt的内容格式如下，以空格间隔：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iPhone差 iPhoneX 100</span><br><span class="line">张旗康 张旗康</span><br></pre></td></tr></table></figure>
<h4 id="加载自定义语言模型"><a href="#加载自定义语言模型" class="headerlink" title="加载自定义语言模型"></a>加载自定义语言模型</h4><p>默认提供下载并使用的kenlm语言模型<code>zh_giga.no_cna_cmn.prune01244.klm</code>文件是2.8G，内存较小的电脑使用<code>pycorrector</code>程序可能会吃力些。</p>
<p>支持用户加载自己训练的kenlm语言模型，或使用2014版人民日报数据训练的模型，模型小（20M），准确率低些。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pycorrector <span class="keyword">import</span> Corrector</span><br><span class="line"></span><br><span class="line">pwd_path = os.path.abspath(os.path.dirname(__file__))</span><br><span class="line">lm_path = os.path.join(pwd_path, <span class="string">&#x27;./people_chars_lm.klm&#x27;</span>)</span><br><span class="line">model = Corrector(language_model_path=lm_path)</span><br><span class="line"></span><br><span class="line">corrected_sent, detail = model.correct(<span class="string">&#x27;少先队员因该为老人让坐&#x27;</span>)</span><br><span class="line">print(corrected_sent, detail)</span><br><span class="line"><span class="comment"># 少先队员应该为老人让座 [[(&#x27;因该&#x27;, &#x27;应该&#x27;, 4, 6)], [(&#x27;坐&#x27;, &#x27;座&#x27;, 10, 11)]]</span></span><br></pre></td></tr></table></figure>
<p>其中<code>./people_chars_lm.klm</code>是自定义语言模型文件。</p>
<h3 id="深度模型"><a href="#深度模型" class="headerlink" title="深度模型"></a>深度模型</h3><p>本项目的初衷之一是比对、共享各种文本纠错方法，抛砖引玉的作用，如果对大家在文本纠错任务上有一点小小的启发就是我莫大的荣幸了。主要使用了多种深度模型应用于文本纠错任务，分别是前面<code>模型</code>小节介绍的<code>seq2seq</code>、<code>transformer</code>、<code>bert</code>、<code>macbert</code>、<code>electra</code>，各模型方法内置于<code>pycorrector</code>文件夹下，有<code>README.md</code>详细指导，各模型可独立运行，相互之间无依赖。</p>
<h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><p>各模型均可独立的预处理数据、训练、预测，下面以其中<code>seq2seq</code>序列生成模型为例。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/shibing624/pycorrector/blob/master/pycorrector/seq2seq">seq2seq</a> 模型使用示例:</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>通过修改<code>config.py</code>。</p>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理??"></a>数据预处理??</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd seq2seq_attention</span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">python preprocess.py</span><br></pre></td></tr></table></figure>
<p>自动新建文件夹output，在output下生成<code>train.txt</code>和<code>test.txt</code>文件，以TAB（”\t”）间隔错误文本和纠正文本，文本以空格切分词，文件内容示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">希 望 少 吸 烟 。	 希 望 烟 民 们 少 吸 烟 。</span><br><span class="line">以 前 ， 包 括 中 国 ， 我 国 也 是 。	以 前 ， 不 仅 中 国 ， 我 国 也 是 。</span><br><span class="line">我 现 在 好 得 多 了 。	我 现 在 好 多 了 。</span><br></pre></td></tr></table></figure>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py</span><br></pre></td></tr></table></figure>
<p>设置<code>config.py</code>中<code>arch=&#39;convseq2seq&#39;</code>，训练sighan数据集（2104条样本），200个epoch，单卡P40GPU训练耗时：3分钟。</p>
<h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python infer.py</span><br></pre></td></tr></table></figure>
<ol>
<li>如果训练数据太少（不足万条），深度模型拟合不足，会出现预测结果全为<code>unk</code>的情况，解决方法：增大训练样本集，使用下方提供的纠错熟语料(nlpcc2018+hsk，130万对句子)测试。</li>
<li>深度模型训练耗时长，有GPU尽量用GPU，加速训练，节省时间。</li>
</ol>
<h4 id="自定义语言模型"><a href="#自定义语言模型" class="headerlink" title="自定义语言模型"></a>自定义语言模型</h4><p>语言模型对于纠错步骤至关重要，当前默认使用的是从千兆中文文本训练的中文语言模型<a target="_blank" rel="noopener" href="https://deepspeech.bj.bcebos.com/zh_lm/zh_giga.no_cna_cmn.prune01244.klm">zh_giga.no_cna_cmn.prune01244.klm(2.8G)</a>。</p>
<p>大家可以用中文维基（繁体转简体，pycorrector.utils.text_utils下有此功能）等语料数据训练通用的语言模型，或者也可以用专业领域语料训练更专用的语言模型。更适用的语言模型，对于纠错效果会有比较好的提升。</p>
<ol>
<li>kenlm语言模型训练工具的使用，请见博客：<a target="_blank" rel="noopener" href="http://blog.csdn.net/mingzai624/article/details/79560063">http://blog.csdn.net/mingzai624/article/details/79560063</a></li>
<li>附上训练语料&lt;人民日报2014版熟语料&gt;，包括： 1）标准人工切词及词性数据people2014.tar.gz， 2）未切词文本数据people2014_words.txt， 3）kenlm训练字粒度语言模型文件及其二进制文件people2014corpus_chars.arps/klm， 4）kenlm词粒度语言模型文件及其二进制文件people2014corpus_words.arps/klm。</li>
</ol>
<h4 id="数据集下载"><a href="#数据集下载" class="headerlink" title="数据集下载"></a>数据集下载</h4><div class="table-container">
<table>
<thead>
<tr>
<th>数据集</th>
<th>语料</th>
<th>下载链接</th>
<th>压缩包大小</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>人民日报2014版语料</code></strong></td>
<td>人民日报2014版</td>
<td><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1971a5XLQsIpL0zL0zxuK2A">百度网盘（密码uc11）</a> <a target="_blank" rel="noopener" href="https://l6pmn3b1eo.feishu.cn/file/boxcnKpildqIseq1D4IrLwlir7c?from=from_qr_code">飞书（密码cHcu）</a></td>
<td>383M</td>
</tr>
<tr>
<td><strong><code>NLPCC 2018 GEC官方数据集</code></strong></td>
<td>NLPCC2018-GEC</td>
<td><a target="_blank" rel="noopener" href="http://tcci.ccf.org.cn/conference/2018/dldoc/trainingdata02.tar.gz">官方trainingdata</a></td>
<td>114M</td>
</tr>
<tr>
<td><strong><code>NLPCC 2018+HSK熟语料</code></strong></td>
<td>nlpcc2018+hsk+CGED</td>
<td><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1BkDru60nQXaDVLRSr7ktfA">百度网盘（密码m6fg）</a> <a target="_blank" rel="noopener" href="https://l6pmn3b1eo.feishu.cn/file/boxcnudJgRs5GEMhZwe77YGTQfc?from=from_qr_code">飞书（密码gl9y）</a></td>
<td>215M</td>
</tr>
<tr>
<td><strong><code>NLPCC 2018+HSK原始语料</code></strong></td>
<td>HSK+Lang8</td>
<td><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1DaOX89uL1JRaZclfrV9C0g">百度网盘（密码n31j）</a> <a target="_blank" rel="noopener" href="https://l6pmn3b1eo.feishu.cn/file/boxcntebW3NI6OAaqzDUXlZHoDb?from=from_qr_code">飞书（密码Q9LH）</a></td>
<td>81M</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>NLPCC 2018 GEC官方数据集<a target="_blank" rel="noopener" href="http://tcci.ccf.org.cn/conference/2018/taskdata.php">NLPCC2018-GEC</a>， 训练集<a target="_blank" rel="noopener" href="http://tcci.ccf.org.cn/conference/2018/dldoc/trainingdata02.tar.gz">trainingdata</a>[解压后114.5MB]，该数据格式是原始文本，未做切词处理。</li>
<li>汉语水平考试（HSK）和lang8原始平行语料[HSK+Lang8]<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1DaOX89uL1JRaZclfrV9C0g">百度网盘（密码n31j）</a>，该数据集已经切词，可用作数据扩增</li>
<li>以上语料，再加上CGED16、CGED17、CGED18的数据，经过以字切分，繁体转简体，打乱数据顺序的预处理后，生成用于纠错的熟语料(nlpcc2018+hsk)，网盘链接:<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1BkDru60nQXaDVLRSr7ktfA">https://pan.baidu.com/s/1BkDru60nQXaDVLRSr7ktfA</a> 密码:m6fg [130万对句子，215MB]</li>
</ol>
<h3 id="功能点"><a href="#功能点" class="headerlink" title="功能点"></a>功能点</h3><ul>
<li>优化形似字字典，提高形似字纠错准确率</li>
<li>整理中文纠错训练数据，使用seq2seq做深度中文纠错模型</li>
<li>添加中文语法错误检测及纠正能力</li>
<li>规则方法添加用户自定义纠错集，并将其纠错优先度调为最高</li>
<li>seq2seq_attention 添加dropout，减少过拟合</li>
<li>在seq2seq模型框架上，新增Pointer-generator network、Beam search、Unknown words replacement、Coverage mechanism等特性</li>
<li>更新bert的fine-tuned使用wiki，适配transformers 2.10.0库</li>
<li>升级代码，兼容TensorFlow 2.0库</li>
<li>升级bert纠错逻辑，提升基于mask的纠错效果</li>
<li>新增基于electra模型的纠错逻辑，参数更小，预测更快</li>
</ul>
<h4 id="2020-12-14-update"><a href="#2020-12-14-update" class="headerlink" title="2020.12.14 update:"></a>2020.12.14 update:</h4><ol>
<li>新增paddle的ERNIE模型用于纠错识别，兼容字粒度和词粒度处理，当前字粒度效果稍好。</li>
<li>规则方法：去掉加载默认的custom_confusion和custom_word_freq，提供设置方法，便于扩展。</li>
<li>新增branch：develop，方便merge新功能。</li>
</ol>
<h4 id="后续优化列表："><a href="#后续优化列表：" class="headerlink" title="后续优化列表："></a>后续优化列表：</h4><ol>
<li>新增专用于纠错任务深度模型，使用bert/ernie预训练模型，加入文本音似、形似特征。</li>
<li>规则方法，改进<code>generate_items</code>疑似错字生成函数，提速并优化逻辑。</li>
<li>预测提速，规则方法加入vertebi动态规划，深度模型使用beamsearch搜索结果，引入GPU + fp16预测部署。</li>
<li>语言模型纠错ppl阈值参数，使用动态调整方法替换写死的阈值。</li>
</ol>
<h1 id="kenlm语言模型"><a href="#kenlm语言模型" class="headerlink" title="kenlm语言模型"></a>kenlm语言模型</h1><h1 id="中文文本纠错算法—错别字纠正的二三事"><a href="#中文文本纠错算法—错别字纠正的二三事" class="headerlink" title="中文文本纠错算法—错别字纠正的二三事"></a>中文文本纠错算法—错别字纠正的二三事</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40806718">https://zhuanlan.zhihu.com/p/40806718</a></p>
<p>训练语言模型的语料中并不clean，包含了很多错别字，这会提高误判率。使用更干净的语料有助于降低误判率，提高正确率。</p>
<p>可扩展性：词典可扩展，可使用自己的语料进行训练，该repo使用的是人民日报数据。扩展性强。</p>
<h1 id="Shaojie2020"><a href="#Shaojie2020" class="headerlink" title="Shaojie2020"></a>Shaojie2020</h1><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/19895141/answer/1650139426">自然语言处理怎么最快入门？</a></p>
<p>More recently, BERT (Devlin et al., 2018), the language representation model, is successfully ap- plied to many language understanding tasks includ- ing CSC (cf., (Hong et al., 2019)). </p>
<p>Attention和Transformer还不熟悉的可以看之前的文章：</p>
<ol>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/43493999">【NLP】Attention原理和源码解析</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44121378">【NLP】Transformer详解</a></p>
</li>
</ol>
<p>BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder.</p>
<p>因为 decoder是不能获得要预测的信息的, 模型的主要创新点都在pre-train方法上: 同乐两种方法分别捕捉词语和句子级别的representation</p>
<ul>
<li>masked LM</li>
<li>next sentence prediction</li>
<li></li>
</ul>
<h2 id="Transformer模型原理详解"><a href="#Transformer模型原理详解" class="headerlink" title="Transformer模型原理详解"></a>Transformer模型原理详解</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/43493999">https://zhuanlan.zhihu.com/p/43493999</a></p>
<p>Attention的思想理解起来比较容易，就是在decoding阶段对input中的信息赋予不同权重。在nlp中就是针对sequence的每个time step input，在cv中就是针对每个pixel。</p>
<h2 id="CSC问题"><a href="#CSC问题" class="headerlink" title="CSC问题"></a>CSC问题</h2><p>Chinese spelling error correction (CSC)</p>
<p>selects a character from a list of candidates for correction (includ- ing non-correction) at each position of the sen- tence on the basis of BERT</p>
<p> BERT does not have sufficient capability to detect whether there is an error at each position,</p>
<p>way of pre-training it using mask language modeling? </p>
<h4 id="李老师的描述"><a href="#李老师的描述" class="headerlink" title="李老师的描述"></a>李老师的描述</h4><p>用户犯错模型</p>
<p>space很大</p>
<p>error 是generate的, 自动生成</p>
<p>distribution</p>
<p>NLP方面可以学习一下BERT</p>
<p>MAB方面就是熟悉一下training的技巧吧</p>
<h4 id="为什么错字没那么容易判断"><a href="#为什么错字没那么容易判断" class="headerlink" title="为什么错字没那么容易判断?"></a>为什么错字没那么容易判断?</h4><ul>
<li>world knowledge: 金字塔 金子塔</li>
<li>inference: 求生欲 求胜欲</li>
</ul>
<p>要是不放在局子里, 那么看起来都是对的</p>
<h4 id="现有的两类方法"><a href="#现有的两类方法" class="headerlink" title="现有的两类方法"></a>现有的两类方法</h4><ul>
<li><p>One employs traditional machine learning and the other deep learning</p>
<p>consisting of a pipeline of error de-</p>
</li>
</ul>
<p><strong>labeled data</strong></p>
<p>via data augmentation, </p>
<p>examples of spelling errors are generated using a large <strong>confusion</strong> table?? </p>
<p>predice the most likely character?</p>
<p>为什么这个方法会有效? — 因为bert的原因?</p>
<h5 id="observation"><a href="#observation" class="headerlink" title="observation"></a>observation</h5><p>error detection capability 还有提高的余地</p>
<p>hypothesize: pre-training BERT with mask language modeling in which only about 15% of the characters in the text are masked<br>only learns the distribution of masked tokens<br>and tends to choose not to make any correction</p>
<h5 id="novel-neural-architecture"><a href="#novel-neural-architecture" class="headerlink" title="novel neural architecture"></a>novel neural architecture</h5><ul>
<li><p>a detection network</p>
<p>solely using BERT</p>
<p>BI-GRU network: predict the prob that the character is an error at each position —&gt; the prob to conduct soft-masking of embedding of chatacter at the position</p>
</li>
<li><p>a correction network</p>
<p>输入是 the soft-masked embedding at each position<br>conduct error using BERT<br>to learn contex for error correction under the help of the detection network, end-to-end joint training?</p>
</li>
</ul>
<h5 id="experiment"><a href="#experiment" class="headerlink" title="experiment"></a>experiment</h5><ul>
<li>benchmark dataset of SIGHAN</li>
<li>create a large and high-quality datasets for valuation ? 这个就是李老师所说的?</li>
</ul>
<h5 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h5><ul>
<li>框架的提出</li>
<li>empirical verification</li>
</ul>
<h2 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h2><p>sequential labeling </p>
<h2 id="3-experiment"><a href="#3-experiment" class="headerlink" title="3 experiment"></a>3 experiment</h2><p>什么是softmask</p>
<h1 id="NLP语言模型"><a href="#NLP语言模型" class="headerlink" title="NLP语言模型"></a>NLP语言模型</h1><h2 id="word2vector"><a href="#word2vector" class="headerlink" title="word2vector"></a>word2vector</h2><p>word2vector已经成为NLP领域的基石算法。<br>原文链接是：<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=http%3A%2F%2Fmccormickml.com%2F2016%2F04%2F19%2Fword2vec-tutorial-the-skip-gram-model%2F">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></p>
<p>word2vector，顾名思义，就是将语料库中的词转化成向量，以便后续在词向量的基础上进行各种计算。最常见的表示方法是counting 编码。假设我们的语料库中是如下三句话：<br>I like deep learning<br>I like NLP<br>I enjoy flying</p>
<p>假设语料库中的单词数量是N，则上图矩阵的大小就是N<em>N，其中的每一行就代表一个词的向量表示。如第一行 [0 2 1 0 0  0 0] 是单词I的向量表示。其中的2代表I这个单词与like这个词在语料库中<em>*共同出现</em></em>  【相邻出现才对吧】了2次。</p>
<p> 似乎我们很简单就完成了“word2vector”是不是？但是这种办法至少有三个缺陷：</p>
<ul>
<li>1是词语数量较大时，向量维度高且稀疏，向量矩阵巨大而难以存储</li>
<li>2是向量并不包含单词的语义内容，只是基于数量统计。</li>
<li>3是当有新的词加入语料库后，整个向量矩阵需要更新</li>
</ul>
<p>尽管我们可以通过SVD来降低向量的维度，但是SVD本身却是一个需要巨大计算量的操作。</p>
<h2 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip gram"></a>skip gram</h2><p>我们今天学习的skip gram算法可以成功克服以上三个缺陷。它的基本思想是首先将所有词语进行one-hot编码，输入只有一个隐藏层的神经网络，定义好loss后进行训练，后面我们会讲解如何定义loss，这里暂时按下不表。训练完成后，我们就可以用隐藏层的权重来作为词的向量表示！！<br> 这个思想乍听起来很神奇是不是？其实我们早就熟悉它了。auto-encoder时，我们也是用有一个隐藏层的神经网络进行训练，训练完成后，丢去后面的output层，只用隐藏层的输出作为最终需要的向量对象，藉此成功完成向量的压缩。</p>
<p>假设我们的语料库中只有一句话：The quick brown fox jumps over the lazy dog.</p>
<p>这句话中共有8个词（这里The与the算同一个词）。<br>skip gram算法是怎么为这8个词生成词向量的呢？</p>
<p>我们知道用神经网络训练，大体有如下几个步骤：</p>
<ul>
<li>准备好data，即X和Y</li>
<li>定义好网络结构</li>
<li>定义好loss</li>
<li>选择合适的优化器</li>
<li>进行迭代训练</li>
<li>存储训练好的网络</li>
</ul>
<h3 id="1-准备X和Y"><a href="#1-准备X和Y" class="headerlink" title="1. 准备X和Y"></a>1. 准备X和Y</h3><p>所以，我们下面先来关注下如何确定X和Y的形式。其实非常简单，（x,y）就是一个个的单词对。比如（the，quick）就是一个单词对，the就是样本数据，quick就是该条样本的标签。 那么，如何从上面那句话中生成单词对数据呢？答案就是n-gram方法。多说不如看图：</p>
<img src="/2021/01/%E8%AF%BB%E6%87%82BERT%E6%A8%A1%E5%9E%8B/image-20210215015407977.png" class="" title="image-20210215015407977">
<p>我们以词为单位扫描这句话，每扫描到一个词，都把该词左右各两个词共4个词拿出来，分别与被扫描的单词组成单词对，作为我们的训练数据。取被扫描单词左右各2个词，这里的2被称为<strong>窗口尺寸</strong>，是可以调整的，用多大的窗口生成的单词对来训练最好，需要具体问题具体分析。一般来说，取5是很好的经验值。也就是左右各取5个单词，共10个单词。这里我们用2只是为了方便说明问题。</p>
<p>以（fox，jumps）为例，jumps可以理解为fox的上下文，我们将fox输入神经网络时，希望网络能够告诉我们，在语料库的8个单词中，jumps是更可能出现在fox周围的。你可能会想，（fox，brown）也是一个单词对，它输入神经网络后，岂不是希望神经网络告诉我们，在8个单词中，brown是更可能出现在fox周围？如果是这样，那么训练完成后的神经网络，输入fox，它的输出会是brown和jumps的哪一个呢？— 答案是取决于（fox，brown）和（fox，jumps）两个单词对谁在训练集中出现的次数比较多，神经网络就会针对哪个单词对按照梯度下降进行更多的调整，从而就会倾向于预测谁将出现在fox周围。</p>
<h3 id="2-数字化表示单词对"><a href="#2-数字化表示单词对" class="headerlink" title="2. 数字化表示单词对"></a>2. 数字化表示单词对</h3><p>上面我们获得了许多单词对作为训练数据，但是神经网络不能直接接收和输出字符串形式的单词对，所以需要将单词对转化为数字的形式。方法也很简单，就是用one-hot编码，如下图所示：</p>
<img src="/2021/01/%E8%AF%BB%E6%87%82BERT%E6%A8%A1%E5%9E%8B/image-20210215020336485.png" class="" title="image-20210215020336485">
<p>（the，quick）单词对就表示成 ((1,0,0,0,0,0,0,0), (0,1,0,0,0,0,0,0))</p>
<p>这样就可以输入神经网络进行训练了，当我们将the输入神经网络时，(1) 希望网络也能输出一个8维的向量，并且第二维尽可能接近1，其他维尽可能接近0。后者是标签嘛. (2) 然，我们还希望这8维向量所有位置的值相加为1，WHY?因为相加为1就可以认为这个8维向量描述的是一个概率分布，正好我们的y值也是一个概率分布（一个位置为1，其他位置为0），我们就可以用交叉熵来衡量神经网络的输出与我们的label y的差异大小，也就可以定义出loss了。</p>
<p>什么，你不知道啥是交叉熵？请参考我的另一篇文章【机器学习面试之各种混乱的熵】，应该不会让你失望。</p>
<h3 id="3-定义网络结构"><a href="#3-定义网络结构" class="headerlink" title="3. 定义网络结构"></a>3. 定义网络结构</h3><p>通过之前的叙述，我们已经基本知道神经网络应该是什么样子了，总结一下，可以确定如下情况：</p>
<ul>
<li>神经网络的输入应该是8维的向量</li>
<li>神经网络只有一个隐藏层</li>
<li><p>神经网络的输出应该是一个8维向量，且各维的值相加为1</p>
<p>它的隐藏层并没有激活函数，但是输出层却用了softmax，这是为了保证输出的向量是一个概率分布。</p>
</li>
</ul>
<h4 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h4><p>训练完之后, 隐藏层这个8x3的矩阵, 每一行就是一个词向量. 【why？？】</p>
<p>so，训练完成后，我们只需要保存好隐藏层的权重矩阵即可，输出层此时已经完成历史使命，可以丢掉了。</p>
<p>那么怎么使用去掉了输出层的网络呢？<br>我们知道，网络的输入是one-hot编码的单词，它与隐藏层权重矩阵相乘实际上是取权重矩阵特定的行，如下图所示：</p>
<p>这意味着，隐藏层实际上相当于是一个查找表，它的输出就是输入的单词的词向量。</p>
<h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>当我们从隐藏层获得一个单词的词向量后，就要经过输出层了。</p>
<p>输出层的神经元数量和语料库中的单词数量一样。</p>
<p>每一个神经元可以认为对应一个单词的<strong>输出权重</strong>，词向量乘以该<strong>输出权重</strong>就得到一个数，该数字代表了输出神经元对应的单词出现在输入单词周围的可能性大小，通过对所有的输出层神经元的输出进行softmax操作，我们就把输出层的输出规整为一个概率分布了。这里有一点需要注意，我们说输出的是该单词出现在输入单词周围的概率大小，这个“周围”包含单词的前面，也包含单词的后面。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/01/%E8%AE%A4%E8%AF%86%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/" rel="prev" title="认识量子计算">
      <i class="fa fa-chevron-left"></i> 认识量子计算
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/01/%E5%BF%83%E6%B5%81%E6%8E%A7%E5%88%B6%E5%88%8D%E8%AE%AE/" rel="next" title="心流控制刍议 [未完,待填坑]">
      心流控制刍议 [未完,待填坑] <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">一些概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">1.0.1.</span> <span class="nav-text">什么是预训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E8%A6%81%E5%81%9A%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="nav-number">1.0.1.1.</span> <span class="nav-text">为什么我们要做预训练模型？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%89%E4%B8%AA%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF"><span class="nav-number">1.0.1.2.</span> <span class="nav-text">预训练模型的三个关键技术</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Transformer"><span class="nav-number">1.0.1.2.1.</span> <span class="nav-text">1 Transformer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.0.1.2.2.</span> <span class="nav-text">2 自监督学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E5%BE%AE%E8%B0%83"><span class="nav-number">1.0.1.2.3.</span> <span class="nav-text">3 微调</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF"><span class="nav-number">1.0.1.2.4.</span> <span class="nav-text">预训练模型发展趋势</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83fine-tuning"><span class="nav-number">1.0.2.</span> <span class="nav-text">什么是模型微调fine tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%E6%97%B6%E5%80%99%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E6%98%AF%E5%90%A6%E6%9B%B4%E6%96%B0%EF%BC%9F"><span class="nav-number">1.0.3.</span> <span class="nav-text">微调时候网络参数是否更新？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fine-tuning-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%89%E7%A7%8D%E7%8A%B6%E6%80%81"><span class="nav-number">1.0.4.</span> <span class="nav-text">fine-tuning 模型的三种状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.0.5.</span> <span class="nav-text">自监督学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bert%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">Bert使用方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A5%E4%B9%8E-XLNet-%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%92%8CBert%E7%9A%84%E5%BC%82%E5%90%8C%E6%AF%94%E8%BE%83"><span class="nav-number">3.</span> <span class="nav-text">知乎: XLNet:运行机制及和Bert的异同比较</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A1%8C%E6%83%85"><span class="nav-number">3.1.</span> <span class="nav-text">行情</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XLNet%E5%BC%95%E5%85%A5%E4%BA%86%E8%87%AA%E5%9B%9E%E5%BD%92%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E8%87%AA%E7%BC%96%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">XLNet引入了自回归语言模型以及自编码语言模型的提法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Autoregressive-LM%EF%BC%89"><span class="nav-number">3.2.0.1.</span> <span class="nav-text">自回归语言模型（Autoregressive LM）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E7%BC%96%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Autoencoder-LM%EF%BC%89"><span class="nav-number">3.2.0.2.</span> <span class="nav-text">自编码语言模型（Autoencoder LM）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XLNet"><span class="nav-number">3.2.0.3.</span> <span class="nav-text">XLNet</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A5%E4%B9%8E-Soft-Masked-BERT%EF%BC%9A%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99%E4%B8%8EBERT%E7%9A%84%E6%9C%80%E6%96%B0%E7%BB%93%E5%90%88"><span class="nav-number">4.</span> <span class="nav-text">知乎: Soft-Masked BERT：文本纠错与BERT的最新结合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80-%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99%E7%A4%BA%E4%BE%8B%E4%B8%8E%E9%9A%BE%E7%82%B9"><span class="nav-number">4.0.1.</span> <span class="nav-text">一.文本纠错示例与难点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C-%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99%E5%B8%B8%E7%94%A8%E6%8A%80%E6%9C%AF"><span class="nav-number">4.0.2.</span> <span class="nav-text">二.文本纠错常用技术</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E5%B0%86BERT%E5%BA%94%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99"><span class="nav-number">4.0.3.</span> <span class="nav-text">三、将BERT应用于文本纠错</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99%E6%9C%80%E4%BC%98%E6%A8%A1%E5%9E%8B%EF%BC%9ASoft-Masked-BERT"><span class="nav-number">4.0.4.</span> <span class="nav-text">四、文本纠错最优模型：Soft-Masked BERT</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#B%E7%AB%99%E4%B8%89%E8%8A%82%E8%AF%BE"><span class="nav-number">5.</span> <span class="nav-text">B站三节课</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CSDN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB"><span class="nav-number">6.</span> <span class="nav-text">CSDN论文解读</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pycorrector"><span class="nav-number">7.</span> <span class="nav-text">Pycorrector</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%84%E5%88%99%E7%9A%84%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF"><span class="nav-number">7.0.1.</span> <span class="nav-text">规则的解决思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF"><span class="nav-number">7.0.2.</span> <span class="nav-text">深度模型的解决思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.0.3.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E6%A3%80%E6%B5%8B"><span class="nav-number">7.0.4.</span> <span class="nav-text">错误检测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E7%BA%A0%E6%AD%A3"><span class="nav-number">7.0.5.</span> <span class="nav-text">错误纠正</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E8%A7%84%E5%88%99%E6%96%B9%E6%B3%95"><span class="nav-number">7.0.6.</span> <span class="nav-text">测试规则方法?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99"><span class="nav-number">7.0.6.1.</span> <span class="nav-text">文本纠错:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E6%A3%80%E6%B5%8B-1"><span class="nav-number">7.0.6.2.</span> <span class="nav-text">错误检测:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%97%AD%E5%AD%97%E7%B2%92%E5%BA%A6%E7%BA%A0%E9%94%99"><span class="nav-number">7.0.6.3.</span> <span class="nav-text">关闭字粒度纠错:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B7%B7%E6%B7%86%E9%9B%86"><span class="nav-number">7.0.6.4.</span> <span class="nav-text">加载自定义混淆集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.0.6.5.</span> <span class="nav-text">加载自定义语言模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.0.7.</span> <span class="nav-text">深度模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">7.0.7.1.</span> <span class="nav-text"> </span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE"><span class="nav-number">7.0.7.2.</span> <span class="nav-text">配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">7.0.7.3.</span> <span class="nav-text">数据预处理??</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">7.0.7.4.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B"><span class="nav-number">7.0.7.5.</span> <span class="nav-text">预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.0.7.6.</span> <span class="nav-text">自定义语言模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD"><span class="nav-number">7.0.7.7.</span> <span class="nav-text">数据集下载</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%9F%E8%83%BD%E7%82%B9"><span class="nav-number">7.0.8.</span> <span class="nav-text">功能点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2020-12-14-update"><span class="nav-number">7.0.8.1.</span> <span class="nav-text">2020.12.14 update:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8E%E7%BB%AD%E4%BC%98%E5%8C%96%E5%88%97%E8%A1%A8%EF%BC%9A"><span class="nav-number">7.0.8.2.</span> <span class="nav-text">后续优化列表：</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kenlm%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.</span> <span class="nav-text">kenlm语言模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99%E7%AE%97%E6%B3%95%E2%80%94%E9%94%99%E5%88%AB%E5%AD%97%E7%BA%A0%E6%AD%A3%E7%9A%84%E4%BA%8C%E4%B8%89%E4%BA%8B"><span class="nav-number">9.</span> <span class="nav-text">中文文本纠错算法—错别字纠正的二三事</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Shaojie2020"><span class="nav-number">10.</span> <span class="nav-text">Shaojie2020</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT"><span class="nav-number">10.1.</span> <span class="nav-text">BERT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3"><span class="nav-number">10.2.</span> <span class="nav-text">Transformer模型原理详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CSC%E9%97%AE%E9%A2%98"><span class="nav-number">10.3.</span> <span class="nav-text">CSC问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%8E%E8%80%81%E5%B8%88%E7%9A%84%E6%8F%8F%E8%BF%B0"><span class="nav-number">10.3.0.1.</span> <span class="nav-text">李老师的描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%94%99%E5%AD%97%E6%B2%A1%E9%82%A3%E4%B9%88%E5%AE%B9%E6%98%93%E5%88%A4%E6%96%AD"><span class="nav-number">10.3.0.2.</span> <span class="nav-text">为什么错字没那么容易判断?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E7%9A%84%E4%B8%A4%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="nav-number">10.3.0.3.</span> <span class="nav-text">现有的两类方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#observation"><span class="nav-number">10.3.0.3.1.</span> <span class="nav-text">observation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#novel-neural-architecture"><span class="nav-number">10.3.0.3.2.</span> <span class="nav-text">novel neural architecture</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#experiment"><span class="nav-number">10.3.0.3.3.</span> <span class="nav-text">experiment</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#contribution"><span class="nav-number">10.3.0.3.4.</span> <span class="nav-text">contribution</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1"><span class="nav-number">10.4.</span> <span class="nav-text">2.1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-experiment"><span class="nav-number">10.5.</span> <span class="nav-text">3 experiment</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLP%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">11.</span> <span class="nav-text">NLP语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#word2vector"><span class="nav-number">11.1.</span> <span class="nav-text">word2vector</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#skip-gram"><span class="nav-number">11.2.</span> <span class="nav-text">skip gram</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%87%86%E5%A4%87X%E5%92%8CY"><span class="nav-number">11.2.1.</span> <span class="nav-text">1. 准备X和Y</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%95%B0%E5%AD%97%E5%8C%96%E8%A1%A8%E7%A4%BA%E5%8D%95%E8%AF%8D%E5%AF%B9"><span class="nav-number">11.2.2.</span> <span class="nav-text">2. 数字化表示单词对</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">11.2.3.</span> <span class="nav-text">3. 定义网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-number">11.2.3.1.</span> <span class="nav-text">隐藏层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-number">11.2.3.2.</span> <span class="nav-text">输出层</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="匠心圆头"
      src="/images/zsy.jpg">
  <p class="site-author-name" itemprop="name">匠心圆头</p>
  <div class="site-description" itemprop="description">致虚极，守静笃。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">124</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/shira0905" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;shira0905" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:shira0905@gmail.com" title="E-Mail → mailto:shira0905@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021-07-08</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">匠心圆头</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
