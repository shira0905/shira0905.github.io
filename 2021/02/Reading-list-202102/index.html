<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=EB Garamond:300,300italic,400,400italic,700,700italic|Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"shira0905.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="这个月主要是NLP, DP, ML等, 为实习做准备.">
<meta property="og:type" content="article">
<meta property="og:title" content="Reading list 202102 (更)*">
<meta property="og:url" content="https://shira0905.github.io/2021/02/Reading-list-202102/">
<meta property="og:site_name" content="圆头修行记">
<meta property="og:description" content="这个月主要是NLP, DP, ML等, 为实习做准备.">
<meta property="og:locale">
<meta property="og:image" content="https://shira0905.github.io/2021/02/Reading-list-202102/image-20210220041300758.png">
<meta property="og:image" content="https://shira0905.github.io/2021/02/Reading-list-202102/image-20210220041004329.png">
<meta property="og:image" content="https://shira0905.github.io/2021/02/Reading-list-202102/image-20210220042714383.png">
<meta property="og:image" content="https://shira0905.github.io/2021/02/Reading-list-202102/image-20210220041004329.png">
<meta property="og:image" content="https://shira0905.github.io/2021/02/Reading-list-202102/image-20210220031230930.png">
<meta property="article:published_time" content="2021-02-19T17:31:23.000Z">
<meta property="article:modified_time" content="2021-02-19T20:30:49.009Z">
<meta property="article:author" content="匠心圆头">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shira0905.github.io/2021/02/Reading-list-202102/image-20210220041300758.png">

<link rel="canonical" href="https://shira0905.github.io/2021/02/Reading-list-202102/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>Reading list 202102 (更)* | 圆头修行记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">圆头修行记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">不自见，故明；不自是，故彰；<br>不自伐，故有功；不自矜，故长。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://shira0905.github.io/2021/02/Reading-list-202102/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zsy.jpg">
      <meta itemprop="name" content="匠心圆头">
      <meta itemprop="description" content="致虚极，守静笃。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圆头修行记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reading list 202102 (更)*
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-02-20 01:31:23 / Modified: 04:30:49" itemprop="dateCreated datePublished" datetime="2021-02-20T01:31:23+08:00">2021-02-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index"><span itemprop="name">Research</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/reading/" itemprop="url" rel="index"><span itemprop="name">reading</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这个月主要是NLP, DP, ML等, 为实习做准备.</p>
<a id="more"></a>
<h1 id="正经书阅读"><a href="#正经书阅读" class="headerlink" title="正经书阅读"></a>正经书阅读</h1><h2 id="概率论与数理统计-在读"><a href="#概率论与数理统计-在读" class="headerlink" title="概率论与数理统计[在读]"></a>概率论与数理统计[在读]</h2><p>RESPECT 向大师致敬.<br>这才是一本教课书该有的样子, 相见恨晚, 感动得有点想哭.<br>羡慕科大的同学本科用的这本教材.</p>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/2201479/">https://book.douban.com/subject/2201479/</a></p>
<img src="/2021/02/Reading-list-202102/image-20210220041300758.png" class="" title="image-20210220041300758">
<h3 id="内容简介"><a href="#内容简介" class="headerlink" title="内容简介"></a>内容简介</h3><p>《概率论与数理统计》内容包括初等概率计算、随机变量及其分布、数字特征、多维随机向量、极限定理、统计学基本概念、点估计与区间估计、假设检验、回归相关分析、方差分析等。书中选入了部分在理论和应用上重要，但一般认为超出本课程范围的材料，以备教者和学者选择。《概率论与数理统计》着重基本概念的阐释，同时，在设定的数学程度内，力求做到论述严谨。书中精选了百余道习题，并在书末附有提示与解答。《概率论与数理统计》可作为高等学校理工科非数学系的概率统计课程教材，也可供具有相当数学准备（初等微积分及少量矩阵知识）的读者自修之用。</p>
<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><p>总序<br>序<br>第1章 事件的概率<br>1.1 概率是什么<br>1.2 古典概率计算<br>1.3 事件的运算、条件概率与独立性<br>习题<br>第2章 随机变量及概率分布<br>2.1 一维随机变量<br>2.2 多维随机变量（随机向量）<br>2.3 条件概率分布与随机变量的独立性<br>2.4 随机变量的函数的概率分布<br>附录<br>习题<br>第3章 随机变量的数字特征<br>3.1 数学期望（均值）与中位数<br>3.2 方差与矩<br>3.3 协方差与相关系数<br>3.2 方差与矩<br>3.3 协方差与相关系数<br>3.4 大数定理和中心极限定理<br>习题<br>第4章 参数估计<br>4.1 数理统计学的基本概念<br>4.2 矩估计、极大似然估计和贝叶斯估计<br>4.3 点估计的优良性准则<br>4.4 区间估计<br>习题<br>第5章 假设检验<br>5.1 问题提法和基本概念<br>5.2 重要参数检验<br>5.3 拟合优度检验<br>附录<br>习题<br>第6章 回归、相关与方差分析<br>6.1 回归分析的基本概念<br>6.2 一元线性回归<br>6.3 多元线性回归<br>6.4 相关分析<br>6.5 方差分析<br>附录<br>习题<br>习题提示与解答<br>附表</p>
<h2 id="自然语言处理入门-在读"><a href="#自然语言处理入门-在读" class="headerlink" title="自然语言处理入门[在读]"></a>自然语言处理入门[在读]</h2><p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/34856701/">https://book.douban.com/subject/34856701/</a></p>
<img src="/2021/02/Reading-list-202102/image-20210220041004329.png" class="" title="image-20210220041004329">
<h3 id="内容简介-1"><a href="#内容简介-1" class="headerlink" title="内容简介"></a>内容简介</h3><p>这是一本务实的入门书，助你零起点上手自然语言处理。</p>
<p>HanLP 作者何晗汇集多年经验，从基本概念出发，逐步介绍中文分词、词性标注、命名实体识别、信 息抽取、文本聚类、文本分类、句法分析这几个热门问题的算法原理与工程实现。书中通过对多种算法的讲解，比较了它们的优缺点和适用场景，同时详细演示生产级成熟代码，助你真正将自然语言处理应用在生产环境中。</p>
<p>随着本书的学习，你将从普通程序员晋级为机器学习工程师，最后进化到自然语言处理工程师。</p>
<h3 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h3><p>何晗（@hankcs）</p>
<p>自然语言处理类库 HanLP 作者（GitHub 加星超过 14 600），“码农场”博主（日活跃读者数超过 3000），埃默里大学计算机博士生，研究方向是句法分析、语义分析与问答系统。</p>
<p>HanLP 和“码农场”是 NLP 领域实用的学习资源，何晗大约每周处理一次 HanLP GitHub上的 Issues。</p>
<h3 id="目录-1"><a href="#目录-1" class="headerlink" title="目录"></a>目录</h3><p>第1章新手上路1<br>1.1自然语言与编程语言.2<br>1.1.1词汇量.2<br>1.1.2结构化.2<br>1.1.3歧义性.3<br>1.1.4容错性.3<br>1.1.5易变性.4<br>1.1.6简略性.4<br>1.2自然语言处理的层次.4<br>1.2.1语音、图像和文本..5<br>1.2.2中文分词、词性标注和命名实体识别.5<br>1.2.3信息抽取.6<br>1.2.4文本分类与文本聚类..6<br>1.2.5句法分析.6<br>1.2.6语义分析与篇章分析..7<br>1.2.7其他高级任务7<br>1.3自然语言处理的流派.8<br>1.3.1基于规则的专家系统..8<br>1.3.2基于统计的学习方法..9<br>1.3.3历史.9<br>1.3.4规则与统计.11<br>1.3.5传统方法与深度学习11<br>1.4机器学习..12<br>1.4.1什么是机器学习13<br>1.4.2模型..13<br>1.4.3特征..13<br>1.4.4数据集..15<br>1.4.5监督学习..16<br>1.4.6无监督学习.17<br>1.4.7其他类型的机器学习算法..18<br>1.5语料库19<br>1.5.1中文分词语料库19<br>1.5.2词性标注语料库19<br>1.5.3命名实体识别语料库20<br>1.5.4句法分析语料库20<br>1.5.5文本分类语料库20<br>1.5.6语料库建设.21<br>1.6开源工具..21<br>1.6.1主流NLP工具比较..21<br>1.6.2Python接口23<br>1.6.3Java接口.28<br>1.7总结.31<br>第2章词典分词32<br>2.1什么是词..32<br>2.1.1词的定义..32<br>2.1.2词的性质—齐夫定律..33<br>2.2词典.34<br>2.2.1HanLP词典.34<br>2.2.2词典的加载.34<br>2.3切分算法..36<br>2.3.1完全切分..36<br>2.3.2正向最长匹配.37<br>2.3.3逆向最长匹配.39<br>2.3.4双向最长匹配.40<br>2.3.5速度评测..43<br>2.4字典树46<br>2.4.1什么是字典树.46<br>2.4.2字典树的节点实现47<br>2.4.3字典树的增删改查实现..48<br>2.4.4首字散列其余二分的字典树.50<br>2.4.5前缀树的妙用.53<br>2.5双数组字典树55<br>2.5.1双数组的定义.55<br>2.5.2状态转移..56<br>2.5.3查询..56<br>2.5.4构造<em>57<br>2.5.5全切分与最长匹配60<br>2.6AC自动机..60<br>2.6.1从字典树到AC自动机61<br>2.6.2goto表61<br>2.6.3output表..62<br>2.6.4fail表63<br>2.6.5实现..65<br>2.7基于双数组字典树的AC自动机.67<br>2.7.1原理..67<br>2.7.2实现..67<br>2.8HanLP的词典分词实现71<br>2.8.1DoubleArrayTrieSegment72<br>2.8.2AhoCorasickDoubleArrayTrie-Segment.73<br>2.9准确率评测.74<br>2.9.1准确率..74<br>2.9.2混淆矩阵与TP/FN/FP/TN..75<br>2.9.3精确率..76<br>2.9.4召回率..76<br>2.9.5F1值..77<br>2.9.6中文分词中的P、R、F1计算..77<br>2.9.7实现..78<br>2.9.8第二届国际中文分词评测..79<br>2.9.9OOVRecallRate与IVRecallRate.81<br>2.10字典树的其他应用.83<br>2.10.1停用词过滤..83<br>2.10.2简繁转换87<br>2.10.3拼音转换90<br>2.11总结.91<br>第3章二元语法与中文分词.92<br>3.1语言模型..92<br>3.1.1什么是语言模型92<br>3.1.2马尔可夫链与二元语法..94<br>3.1.3n元语法..95<br>3.1.4数据稀疏与平滑策略96<br>3.2中文分词语料库.96<br>3.2.11998年《人民日报》语料库PKU.97<br>3.2.2微软亚洲研究院语料库MSR98<br>3.2.3繁体中文分词语料库98<br>3.2.4语料库统计.99<br>3.3训练.100<br>3.3.1加载语料库..101<br>3.3.2统计一元语法..101<br>3.3.3统计二元语法..103<br>3.4预测..104<br>3.4.1加载模型104<br>3.4.2构建词网107<br>3.4.3节点间的距离计算111<br>3.4.4词图上的维特比算法.112<br>3.4.5与用户词典的集成115<br>3.5评测..118<br>3.5.1标准化评测..118<br>3.5.2误差分析118<br>3.5.3调整模型119<br>3.6日语分词122<br>3.6.1日语分词语料..122<br>3.6.2训练日语分词器.123<br>3.7总结..124<br>第4章隐马尔可夫模型与序列标注.125<br>4.1序列标注问题.125<br>4.1.1序列标注与中文分词.126<br>4.1.2序列标注与词性标注.127<br>4.1.3序列标注与命名实体识别128<br>4.2隐马尔可夫模型..129<br>4.2.1从马尔可夫假设到隐马尔可夫模型129<br>4.2.2初始状态概率向量.130<br>4.2.3状态转移概率矩阵.131<br>4.2.4发射概率矩阵..132<br>4.2.5隐马尔可夫模型的三个基本用法..133<br>4.3隐马尔可夫模型的样本生成133<br>4.3.1案例—医疗诊断.133<br>4.3.2样本生成算法..136<br>4.4隐马尔可夫模型的训练..138<br>4.4.1转移概率矩阵的估计.138<br>4.4.2初始状态概率向量的估计139<br>4.4.3发射概率矩阵的估计.140<br>4.4.4验证样本生成与模型训练141<br>4.5隐马尔可夫模型的预测..142<br>4.5.1概率计算的前向算法.142<br>4.5.2搜索状态序列的维特比算法..143<br>4.6隐马尔可夫模型应用于中文分词.147<br>4.6.1标注集148<br>4.6.2字符映射149<br>4.6.3语料转换150<br>4.6.4训练151<br>4.6.5预测152<br>4.6.6评测153<br>4.6.7误差分析154<br>4.7二阶隐马尔可夫模型</em>154<br>4.7.1二阶转移概率张量的估计155<br>4.7.2二阶隐马尔可夫模型中的维特比算法156<br>4.7.3二阶隐马尔可夫模型应用于中文分词158<br>4.8总结..159<br>第5章感知机分类与序列标注.160<br>5.1分类问题160<br>5.1.1定义160<br>5.1.2应用161<br>5.2线性分类模型与感知机算法161<br>5.2.1特征向量与样本空间.162<br>5.2.2决策边界与分离超平面164<br>5.2.3感知机算法..167<br>5.2.4损失函数与随机梯度下降<em>169<br>5.2.5投票感知机和平均感知机171<br>5.3基于感知机的人名性别分类174<br>5.3.1人名性别语料库.174<br>5.3.2特征提取174<br>5.3.3训练175<br>5.3.4预测176<br>5.3.5评测177<br>5.3.6模型调优178<br>5.4结构化预测问题..180<br>5.4.1定义180<br>5.4.2结构化预测与学习的流程180<br>5.5线性模型的结构化感知机算法..180<br>5.5.1结构化感知机算法.180<br>5.5.2结构化感知机与序列标注182<br>5.5.3结构化感知机的维特比解码算法..183<br>5.6基于结构化感知机的中文分词..186<br>5.6.1特征提取187<br>5.6.2多线程训练..189<br>5.6.3特征裁剪与模型压缩</em>.190<br>5.6.4创建感知机分词器.192<br>5.6.5准确率与性能..194<br>5.6.6模型调整与在线学习<em>.195<br>5.6.7中文分词特征工程</em>.197<br>5.7总结..199<br>第6章条件随机场与序列标注.200<br>6.1机器学习的模型谱系200<br>6.1.1生成式模型与判别式模型201<br>6.1.2有向与无向概率图模型202<br>6.2条件随机场..205<br>6.2.1线性链条件随机场.205<br>6.2.2条件随机场的训练<em>207<br>6.2.3对比结构化感知机.210<br>6.3条件随机场工具包.212<br>6.3.1CRF++的安装212<br>6.3.2CRF++语料格式213<br>6.3.3CRF++特征模板214<br>6.3.4CRF++命令行训练215<br>6.3.5CRF++模型格式</em>216<br>6.3.6CRF++命令行预测217<br>6.3.7CRF++代码分析*218<br>6.4HanLP中的CRF++API220<br>6.4.1训练分词器..220<br>6.4.2标准化评测..220<br>6.5总结..221<br>第7章词性标注.222<br>7.1词性标注概述.222<br>7.1.1什么是词性..222<br>7.1.2词性的用处..223<br>7.1.3词性标注223<br>7.1.4词性标注模型..223<br>7.2词性标注语料库与标注集.224<br>7.2.1《人民日报》语料库与PKU标注集..225<br>7.2.2国家语委语料库与863标注集.231<br>7.2.3《诛仙》语料库与CTB标注集..234<br>7.3序列标注模型应用于词性标注..236<br>7.3.1基于隐马尔可夫模型的词性标注..237<br>7.3.2基于感知机的词性标注238<br>7.3.3基于条件随机场的词性标注..240<br>7.3.4词性标注评测..241<br>7.4自定义词性..242<br>7.4.1朴素实现242<br>7.4.2标注语料243<br>7.5总结..244<br>第8章命名实体识别.245<br>8.1概述..245<br>8.2基于规则的命名实体识别.246<br>8.3命名实体识别语料库..250<br>8.4基于层叠隐马尔可夫模型的角色标注框架252<br>8.5基于序列标注的命名实体识别..260<br>8.6自定义领域命名实体识别.266<br>8.7总结..268<br>第9章信息抽取.270<br>9.1新词提取270<br>9.2关键词提取..276<br>9.3短语提取283<br>9.4关键句提取..284<br>9.5总结..287<br>第10章文本聚类.288<br>10.1概述..288<br>10.2文档的特征提取291<br>10.3k均值算法293<br>10.4重复二分聚类算法..300<br>10.5标准化评测..303<br>10.6总结..305<br>第11章文本分类.306<br>11.1文本分类的概念306<br>11.2文本分类语料库307<br>11.3文本分类的特征提取.308<br>11.4朴素贝叶斯分类器..312<br>11.5支持向量机分类器..317<br>11.6标准化评测..320<br>11.7情感分析321<br>11.8总结..323<br>第12章依存句法分析.324<br>12.1短语结构树..324<br>12.1.3宾州树库和中文树库.326<br>12.2依存句法树..327<br>12.3依存句法分析.333<br>12.4基于转移的依存句法分析..334<br>12.5依存句法分析API340<br>12.6案例：基于依存句法树的意见抽取..342<br>12.7总结..344<br>第13章深度学习与自然语言处理345<br>13.1传统方法的局限345<br>13.2深度学习与优势348<br>13.3word2vec..353<br>13.4基于神经网络的高性能依存句法分析器.360<br>13.5自然语言处理进阶..363<br>自然语言处理学习资料推荐..365</p>
<h2 id="神经网络与深度学习-想读"><a href="#神经网络与深度学习-想读" class="headerlink" title="神经网络与深度学习[想读]"></a>神经网络与深度学习[想读]</h2><p>在网上有提供免费的pdf版本</p>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/35044046/">https://book.douban.com/subject/35044046/</a></p>
<img src="/2021/02/Reading-list-202102/image-20210220042714383.png" class="" title="image-20210220042714383">
<h3 id="内容简介-2"><a href="#内容简介-2" class="headerlink" title="内容简介"></a>内容简介</h3><p>本书主要介绍神经网络与深度学习中的基础知识、主要模型（卷积神经网络、递归神经网络等）以及在计算机视觉、自然语言处理等领域的应用。</p>
<h3 id="作者简介-1"><a href="#作者简介-1" class="headerlink" title="作者简介"></a>作者简介</h3><p>邱锡鹏复旦大学计算机科学技术学院教授、博士生导师，于复旦大学获得理学学士和博士学位。主要研究领域包括自然语言处理、机器学习、深度学习等，在相关领域的权威国际期刊、会议上发表学术论文60余篇，获得计算语言学顶级国际会议ACL 2017杰出论文奖、全国计算语言学会议CCL 2019最佳论文奖，2015年入选首届中国科协青年人才托举工程，2018年获得中国中文信息学会“钱伟长中文信息处理科学技术奖青年创新一等奖”，入选由“清华—中国工程院知识智能联合研究中心和清华大学人工智能研究院”联合发布的2020年人工智能（AI）全球最具影响力学者提名。该排名参考过去十年人工智能各子领域最有影响力的会议和期刊发表论文的引用情况，排名前10的学者当选该领域当年最具影响力学者奖，排名前100的其他学者获最具影响力学者提名奖。作为项目负责人开源发布了两个自然语言处理开源系统FudanNLP和FastNLP，获得了学术界和产业界的广泛使用。目前担任中国中文信息学会青年工作委员会执行委员、计算语言学专委会委员、语言与知识计算专委会委员，中国人工智能学会青年工作委员会常务委员、自然语言理解专委会委员。</p>
<h3 id="目录-2"><a href="#目录-2" class="headerlink" title="目录"></a>目录</h3><p>序<br>前言<br>常用符号表<br>第一部分 机器学习基础<br>第1章 绪论3<br>1.1人工智能………………………….4<br>1.1.1人工智能的发展历史………………..5<br>1.1.2人工智能的流派…………………..7<br>1.2机器学习………………………….7<br>1.3表示学习………………………….8<br>1.3.1局部表示和分布式表示……………….9<br>1.3.2表示学习………………………11<br>1.4深度学习………………………….11<br>1.4.1端到端学习……………………..12<br>1.5神经网络………………………….13<br>1.5.1人脑神经网络……………………13<br>1.5.2人工神经网络……………………14<br>1.5.3神经网络的发展历史………………..15<br>1.6本书的知识体系………………………17<br>1.7常用的深度学习框架…………………….18<br>1.8总结和深入阅读………………………20<br>第2章 机器学习概述23<br>2.1基本概念………………………….24<br>2.2机器学习的三个基本要素………………….26<br>2.2.1模型…………………………26<br>2.2.2学习准则………………………27<br>2.2.3优化算法………………………30<br>2.3机器学习的简单示例——线性回归……………..33<br>2.3.1参数学习………………………34<br>2.4偏差-方差分解……………………….38<br>2.5机器学习算法的类型…………………….41<br>2.6数据的特征表示………………………43<br>2.6.1传统的特征学习…………………..44<br>2.6.2深度学习方法……………………46<br>2.7评价指标………………………….46<br>2.8理论和定理…………………………49<br>2.8.1PAC学习理论……………………49<br>2.8.2没有免费午餐定理………………….50<br>2.8.3奥卡姆剃刀原理…………………..50<br>2.8.4丑小鸭定理……………………..51<br>2.8.5归纳偏置………………………51<br>2.9总结和深入阅读………………………51<br>第3章 线性模型<br>3.1线性判别函数和决策边界………………….56<br>3.1.1二分类……………………….56<br>3.1.2多分类……………………….58<br>3.2Logistic回归………………………..59<br>3.2.1参数学习………………………60<br>3.3Softmax回归………………………..61<br>3.3.1参数学习………………………62<br>3.4感知器……………………………64<br>3.4.1参数学习………………………64<br>3.4.2感知器的收敛性…………………..66<br>3.4.3参数平均感知器…………………..67<br>3.4.4扩展到多分类……………………69<br>3.5支持向量机…………………………71<br>3.5.1参数学习………………………73<br>3.5.2核函数……………………….74<br>3.5.3软间隔……………………….74<br>3.6损失函数对比………………………..75<br>3.7总结和深入阅读………………………76<br>第二部分 基础模型<br>第4章 前馈神经网络81<br>4.1神经元……………………………82<br>4.1.1Sigmoid型函数…………………..83<br>4.1.2ReLU函数……………………..86<br>4.1.3Swish函数……………………..88<br>4.1.4GELU函数……………………..89<br>4.1.5Maxout单元…………………….89<br>4.2网络结构………………………….90<br>4.2.1前馈网络………………………90<br>4.2.2记忆网络………………………90<br>4.2.3图网络……………………….90<br>4.3前馈神经网络………………………..91<br>4.3.1通用近似定理……………………93<br>4.3.2应用到机器学习…………………..94<br>4.3.3参数学习………………………95<br>4.4反向传播算法………………………..95<br>4.5自动梯度计算………………………..98<br>4.5.1数值微分………………………99<br>4.5.2符号微分………………………99<br>4.5.3自动微分………………………100<br>4.6优化问题………………………….103<br>4.6.1非凸优化问题……………………103<br>4.6.2梯度消失问题……………………104<br>4.7总结和深入阅读………………………104<br>第5章 卷积神经网络109<br>5.1卷积…………………………….110<br>5.1.1卷积的定义……………………..110<br>5.1.2互相关……………………….112<br>5.1.3卷积的变种……………………..113<br>5.1.4卷积的数学性质…………………..114<br>5.2卷积神经网络………………………..115<br>5.2.1用卷积来代替全连接………………..115<br>5.2.2卷积层……………………….116<br>5.2.3汇聚层……………………….118<br>5.2.4卷积网络的整体结构………………..119<br>5.3参数学习………………………….120<br>5.3.1卷积神经网络的反向传播算法……………120<br>5.4几种典型的卷积神经网络………………….121<br>5.4.1LeNet-5……………………….122<br>5.4.2AlexNet………………………123<br>5.4.3Inception网络……………………125<br>5.4.4残差网络………………………126<br>5.5其他卷积方式………………………..127<br>5.5.1转置卷积………………………127<br>5.5.2空洞卷积………………………129<br>5.6总结和深入阅读………………………130<br>第6章 循环神经网络133<br>6.1给网络增加记忆能力…………………….134<br>6.1.1延时神经网络……………………134<br>6.1.2有外部输入的非线性自回归模型…………..134<br>6.1.3循环神经网络……………………135<br>6.2简单循环网络………………………..135<br>6.2.1循环神经网络的计算能力………………136<br>6.3应用到机器学习………………………138<br>6.3.1序列到类别模式…………………..138<br>6.3.2同步的序列到序列模式……………….139<br>6.3.3异步的序列到序列模式……………….139<br>6.4参数学习………………………….140<br>6.4.1随时间反向传播算法………………..141<br>6.4.2实时循环学习算法………………….142<br>6.5长程依赖问题………………………..143<br>6.5.1改进方案………………………144<br>6.6基于门控的循环神经网络………………….145<br>6.6.1长短期记忆网络…………………..145<br>6.6.2LSTM网络的各种变体……………….147<br>6.6.3门控循环单元网络………………….148<br>6.7深层循环神经网络……………………..149<br>6.7.1堆叠循环神经网络………………….150<br>6.7.2双向循环神经网络………………….150<br>6.8扩展到图结构………………………..151<br>6.8.1递归神经网络……………………151<br>6.8.2图神经网络……………………..152<br>6.9总结和深入阅读………………………153<br>第7章 网络优化与正则化157<br>7.1网络优化………………………….157<br>7.1.1网络结构多样性…………………..158<br>7.1.2高维变量的非凸优化………………..158<br>7.1.3神经网络优化的改善方法………………160<br>7.2优化算法………………………….160<br>7.2.1小批量梯度下降…………………..160<br>7.2.2批量大小选择……………………161<br>7.2.3学习率调整……………………..162<br>7.2.4梯度估计修正……………………167<br>7.2.5优化算法小结……………………170<br>7.3参数初始化…………………………171<br>7.3.1基于固定方差的参数初始化……………..172<br>7.3.2基于方差缩放的参数初始化……………..173<br>7.3.3正交初始化……………………..175<br>7.4数据预处理…………………………176<br>7.5逐层归一化…………………………178<br>7.5.1批量归一化……………………..179<br>7.5.2层归一化………………………181<br>7.5.3权重归一化……………………..182<br>7.5.4局部响应归一化…………………..182<br>7.6超参数优化…………………………183<br>7.6.1网格搜索………………………183<br>7.6.2随机搜索………………………184<br>7.6.3贝叶斯优化……………………..184<br>7.6.4动态资源分配……………………185<br>7.6.5神经架构搜索……………………186<br>7.7网络正则化…………………………186<br>7.7.1?1和?2正则化……………………187<br>7.7.2权重衰减………………………188<br>7.7.3提前停止………………………188<br>7.7.4丢弃法……………………….189<br>7.7.5数据增强………………………191<br>7.7.6标签平滑………………………191<br>7.8总结和深入阅读………………………192<br>第8章 注意力机制与外部记忆197<br>8.1认知神经学中的注意力…………………..198<br>8.2注意力机制…………………………199<br>8.2.1注意力机制的变体………………….201<br>8.3自注意力模型………………………..203<br>8.4人脑中的记忆………………………..205<br>8.5记忆增强神经网络……………………..207<br>8.5.1端到端记忆网络…………………..208<br>8.5.2神经图灵机……………………..210<br>8.6基于神经动力学的联想记忆…………………211<br>8.6.1Hopfiel网络……………………212<br>8.6.2使用联想记忆增加网络容量……………..215<br>8.7总结和深入阅读………………………215<br>第9章 无监督学习219<br>9.1无监督特征学习………………………220<br>9.1.1主成分分析……………………..220<br>9.1.2稀疏编码………………………222<br>9.1.3自编码器………………………224<br>9.1.4稀疏自编码器……………………225<br>9.1.5堆叠自编码器……………………226<br>9.1.6降噪自编码器……………………226<br>9.2概率密度估计………………………..227<br>9.2.1参数密度估计……………………227<br>9.2.2非参数密度估计…………………..229<br>9.3总结和深入阅读………………………232<br>第10章 模型独立的学习方式235<br>10.1集成学习………………………….235<br>10.1.1AdaBoost算法……………………237<br>10.2自训练和协同训练……………………..240<br>10.2.1自训练……………………….240<br>10.2.2协同训练………………………240<br>10.3多任务学习…………………………242<br>10.4迁移学习………………………….245<br>10.4.1归纳迁移学习……………………246<br>10.4.2转导迁移学习……………………247<br>10.5终身学习………………………….249<br>10.6元学习……………………………252<br>10.6.1基于优化器的元学习………………..253<br>10.6.2模型无关的元学习………………….254<br>10.7总结和深入阅读………………………255<br>第三部分 进阶模型<br>第11章 概率图模型261<br>11.1模型表示………………………….262<br>11.1.1有向图模型……………………..263<br>11.1.2常见的有向图模型………………….264<br>11.1.3无向图模型……………………..267<br>11.1.4无向图模型的概率分解……………….267<br>11.1.5常见的无向图模型………………….269<br>11.1.6有向图和无向图之间的转换……………..270<br>11.2学习…………………………….271<br>11.2.1不含隐变量的参数估计……………….271<br>11.2.2含隐变量的参数估计………………..273<br>11.3推断…………………………….279<br>11.3.1精确推断………………………279<br>11.3.2近似推断………………………282<br>11.4变分推断………………………….283<br>11.5基于采样法的近似推断…………………..285<br>11.5.1采样法……………………….285<br>11.5.2拒绝采样………………………287<br>11.5.3重要性采样……………………..288<br>11.5.4马尔可夫链蒙特卡罗方法………………289<br>11.6总结和深入阅读………………………292<br>第12章 深度信念网络297<br>12.1玻尔兹曼机…………………………297<br>12.1.1生成模型………………………299<br>12.1.2能量最小化与模拟退火……………….301<br>12.1.3参数学习………………………302<br>12.2受限玻尔兹曼机………………………304<br>12.2.1生成模型………………………305<br>12.2.2参数学习………………………307<br>12.2.3受限玻尔兹曼机的类型……………….308<br>12.3深度信念网络………………………..309<br>12.3.1生成模型………………………310<br>12.3.2参数学习………………………310<br>12.4总结和深入阅读………………………313<br>第13章 深度生成模型317<br>13.1概率生成模型………………………..318<br>13.1.1密度估计………………………318<br>13.1.2生成样本………………………319<br>13.1.3应用于监督学习…………………..319<br>13.2变分自编码器………………………..319<br>13.2.1含隐变量的生成模型………………..319<br>13.2.2推断网络………………………321<br>13.2.3生成网络………………………323<br>13.2.4模型汇总………………………323<br>13.2.5再参数化………………………325<br>13.2.6训练…………………………325<br>13.3生成对抗网络………………………..327<br>13.3.1显式密度模型和隐式密度模型……………327<br>13.3.2网络分解………………………327<br>13.3.3训练…………………………329<br>13.3.4一个生成对抗网络的具体实现：DCGAN……….330<br>13.3.5模型分析………………………330<br>13.3.6改进模型………………………333<br>13.4总结和深入阅读………………………336<br>第14章 深度强化学习339<br>14.1强化学习问题………………………..340<br>14.1.1典型例子………………………340<br>14.1.2强化学习定义……………………340<br>14.1.3马尔可夫决策过程………………….341<br>14.1.4强化学习的目标函数………………..343<br>14.1.5值函数……………………….344<br>14.1.6深度强化学习……………………345<br>14.2基于值函数的学习方法…………………..346<br>14.2.1动态规划算法……………………346<br>14.2.2蒙特卡罗方法……………………349<br>14.2.3时序差分学习方法………………….350<br>14.2.4深度Q网络……………………..353<br>14.3基于策略函数的学习方法………………….354<br>14.3.1REINFORCE算法………………….356<br>14.3.2带基准线的REINFORCE算法……………356<br>14.4演员-评论员算法………………………358<br>14.5总结和深入阅读………………………360<br>第15章 序列生成模型365<br>15.1序列概率模型………………………..366<br>15.1.1序列生成………………………367<br>15.2N元统计模型………………………..368<br>15.3深度序列模型………………………..370<br>15.3.1模型结构………………………370<br>15.3.2参数学习………………………373<br>15.4评价方法………………………….373<br>15.4.1困惑度……………………….373<br>15.4.2BLEU算法……………………..374<br>15.4.3ROUGE算法…………………….375<br>15.5序列生成模型中的学习问题…………………375<br>15.5.1曝光偏差问题……………………376<br>15.5.2训练目标不一致问题………………..377<br>15.5.3计算效率问题……………………377<br>15.6序列到序列模型………………………385<br>15.6.1基于循环神经网络的序列到序列模型………..386<br>15.6.2基于注意力的序列到序列模型……………387<br>15.6.3基于自注意力的序列到序列模型…………..388<br>15.7总结和深入阅读………………………390<br>附录数学基础 393<br>附录A 线性代数 394<br>附录B 微积分 404<br>附录C 数学优化 413<br>附录D 概率论 420<br>附录E 信息论 433<br>索引 439</p>
<h1 id="闲书阅读"><a href="#闲书阅读" class="headerlink" title="闲书阅读"></a>闲书阅读</h1><h2 id="心流-完"><a href="#心流-完" class="headerlink" title="心流[完]"></a>心流[完]</h2><p>带点道家思想的影子. 作者的很多观点我双手赞同.<br>又是值得独立开贴写感想的书, 等有时间有心情再写吧, 已占坑位”心流刍议”.</p>
<h2 id="历史深处的忧虑-完"><a href="#历史深处的忧虑-完" class="headerlink" title="历史深处的忧虑[完]"></a>历史深处的忧虑[完]</h2><p>很喜欢林达的文笔, 严谨又很轻松. 也很少批判某种制度, 对各种观点都很包容.<br>感触很多, 展开写估计又是长篇大论了.<br>最大的感触是: 在中国长大的人确实是很难融入美国社会, </p>
<h2 id="声音的魅力-完"><a href="#声音的魅力-完" class="headerlink" title="声音的魅力[完]"></a>声音的魅力[完]</h2><p>有很多实用的技巧, 而且有些是立竿见影的, 总结在”__圆头语录2021”里面了.<br>勤加练习!</p>
<h2 id="中国哲学简史-在读"><a href="#中国哲学简史-在读" class="headerlink" title="中国哲学简史[在读]"></a>中国哲学简史[在读]</h2><p>作者: 冯友兰</p>
<h2 id="道德经下篇-在读"><a href="#道德经下篇-在读" class="headerlink" title="道德经下篇[在读]"></a>道德经下篇[在读]</h2><h2 id="为什么要睡觉-在读"><a href="#为什么要睡觉-在读" class="headerlink" title="为什么要睡觉[在读]"></a>为什么要睡觉[在读]</h2><h1 id="正经书购入"><a href="#正经书购入" class="headerlink" title="正经书购入"></a>正经书购入</h1><h2 id="自然语言处理入门-已购"><a href="#自然语言处理入门-已购" class="headerlink" title="自然语言处理入门[已购]"></a>自然语言处理入门[已购]</h2><p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/34856701/">https://book.douban.com/subject/34856701/</a></p>
<img src="/2021/02/Reading-list-202102/image-20210220041004329.png" class="" title="image-20210220041004329">
<h3 id="内容简介-3"><a href="#内容简介-3" class="headerlink" title="内容简介"></a>内容简介</h3><p>这是一本务实的入门书，助你零起点上手自然语言处理。</p>
<p>HanLP 作者何晗汇集多年经验，从基本概念出发，逐步介绍中文分词、词性标注、命名实体识别、信 息抽取、文本聚类、文本分类、句法分析这几个热门问题的算法原理与工程实现。书中通过对多种算法的讲解，比较了它们的优缺点和适用场景，同时详细演示生产级成熟代码，助你真正将自然语言处理应用在生产环境中。</p>
<p>随着本书的学习，你将从普通程序员晋级为机器学习工程师，最后进化到自然语言处理工程师。</p>
<h3 id="作者简介-2"><a href="#作者简介-2" class="headerlink" title="作者简介"></a>作者简介</h3><p>何晗（@hankcs）</p>
<p>自然语言处理类库 HanLP 作者（GitHub 加星超过 14 600），“码农场”博主（日活跃读者数超过 3000），埃默里大学计算机博士生，研究方向是句法分析、语义分析与问答系统。</p>
<p>HanLP 和“码农场”是 NLP 领域实用的学习资源，何晗大约每周处理一次 HanLP GitHub上的 Issues。</p>
<h3 id="目录-3"><a href="#目录-3" class="headerlink" title="目录"></a>目录</h3><p>第1章新手上路1<br>1.1自然语言与编程语言.2<br>1.1.1词汇量.2<br>1.1.2结构化.2<br>1.1.3歧义性.3<br>1.1.4容错性.3<br>1.1.5易变性.4<br>1.1.6简略性.4<br>1.2自然语言处理的层次.4<br>1.2.1语音、图像和文本..5<br>1.2.2中文分词、词性标注和命名实体识别.5<br>1.2.3信息抽取.6<br>1.2.4文本分类与文本聚类..6<br>1.2.5句法分析.6<br>1.2.6语义分析与篇章分析..7<br>1.2.7其他高级任务7<br>1.3自然语言处理的流派.8<br>1.3.1基于规则的专家系统..8<br>1.3.2基于统计的学习方法..9<br>1.3.3历史.9<br>1.3.4规则与统计.11<br>1.3.5传统方法与深度学习11<br>1.4机器学习..12<br>1.4.1什么是机器学习13<br>1.4.2模型..13<br>1.4.3特征..13<br>1.4.4数据集..15<br>1.4.5监督学习..16<br>1.4.6无监督学习.17<br>1.4.7其他类型的机器学习算法..18<br>1.5语料库19<br>1.5.1中文分词语料库19<br>1.5.2词性标注语料库19<br>1.5.3命名实体识别语料库20<br>1.5.4句法分析语料库20<br>1.5.5文本分类语料库20<br>1.5.6语料库建设.21<br>1.6开源工具..21<br>1.6.1主流NLP工具比较..21<br>1.6.2Python接口23<br>1.6.3Java接口.28<br>1.7总结.31<br>第2章词典分词32<br>2.1什么是词..32<br>2.1.1词的定义..32<br>2.1.2词的性质—齐夫定律..33<br>2.2词典.34<br>2.2.1HanLP词典.34<br>2.2.2词典的加载.34<br>2.3切分算法..36<br>2.3.1完全切分..36<br>2.3.2正向最长匹配.37<br>2.3.3逆向最长匹配.39<br>2.3.4双向最长匹配.40<br>2.3.5速度评测..43<br>2.4字典树46<br>2.4.1什么是字典树.46<br>2.4.2字典树的节点实现47<br>2.4.3字典树的增删改查实现..48<br>2.4.4首字散列其余二分的字典树.50<br>2.4.5前缀树的妙用.53<br>2.5双数组字典树55<br>2.5.1双数组的定义.55<br>2.5.2状态转移..56<br>2.5.3查询..56<br>2.5.4构造<em>57<br>2.5.5全切分与最长匹配60<br>2.6AC自动机..60<br>2.6.1从字典树到AC自动机61<br>2.6.2goto表61<br>2.6.3output表..62<br>2.6.4fail表63<br>2.6.5实现..65<br>2.7基于双数组字典树的AC自动机.67<br>2.7.1原理..67<br>2.7.2实现..67<br>2.8HanLP的词典分词实现71<br>2.8.1DoubleArrayTrieSegment72<br>2.8.2AhoCorasickDoubleArrayTrie-Segment.73<br>2.9准确率评测.74<br>2.9.1准确率..74<br>2.9.2混淆矩阵与TP/FN/FP/TN..75<br>2.9.3精确率..76<br>2.9.4召回率..76<br>2.9.5F1值..77<br>2.9.6中文分词中的P、R、F1计算..77<br>2.9.7实现..78<br>2.9.8第二届国际中文分词评测..79<br>2.9.9OOVRecallRate与IVRecallRate.81<br>2.10字典树的其他应用.83<br>2.10.1停用词过滤..83<br>2.10.2简繁转换87<br>2.10.3拼音转换90<br>2.11总结.91<br>第3章二元语法与中文分词.92<br>3.1语言模型..92<br>3.1.1什么是语言模型92<br>3.1.2马尔可夫链与二元语法..94<br>3.1.3n元语法..95<br>3.1.4数据稀疏与平滑策略96<br>3.2中文分词语料库.96<br>3.2.11998年《人民日报》语料库PKU.97<br>3.2.2微软亚洲研究院语料库MSR98<br>3.2.3繁体中文分词语料库98<br>3.2.4语料库统计.99<br>3.3训练.100<br>3.3.1加载语料库..101<br>3.3.2统计一元语法..101<br>3.3.3统计二元语法..103<br>3.4预测..104<br>3.4.1加载模型104<br>3.4.2构建词网107<br>3.4.3节点间的距离计算111<br>3.4.4词图上的维特比算法.112<br>3.4.5与用户词典的集成115<br>3.5评测..118<br>3.5.1标准化评测..118<br>3.5.2误差分析118<br>3.5.3调整模型119<br>3.6日语分词122<br>3.6.1日语分词语料..122<br>3.6.2训练日语分词器.123<br>3.7总结..124<br>第4章隐马尔可夫模型与序列标注.125<br>4.1序列标注问题.125<br>4.1.1序列标注与中文分词.126<br>4.1.2序列标注与词性标注.127<br>4.1.3序列标注与命名实体识别128<br>4.2隐马尔可夫模型..129<br>4.2.1从马尔可夫假设到隐马尔可夫模型129<br>4.2.2初始状态概率向量.130<br>4.2.3状态转移概率矩阵.131<br>4.2.4发射概率矩阵..132<br>4.2.5隐马尔可夫模型的三个基本用法..133<br>4.3隐马尔可夫模型的样本生成133<br>4.3.1案例—医疗诊断.133<br>4.3.2样本生成算法..136<br>4.4隐马尔可夫模型的训练..138<br>4.4.1转移概率矩阵的估计.138<br>4.4.2初始状态概率向量的估计139<br>4.4.3发射概率矩阵的估计.140<br>4.4.4验证样本生成与模型训练141<br>4.5隐马尔可夫模型的预测..142<br>4.5.1概率计算的前向算法.142<br>4.5.2搜索状态序列的维特比算法..143<br>4.6隐马尔可夫模型应用于中文分词.147<br>4.6.1标注集148<br>4.6.2字符映射149<br>4.6.3语料转换150<br>4.6.4训练151<br>4.6.5预测152<br>4.6.6评测153<br>4.6.7误差分析154<br>4.7二阶隐马尔可夫模型</em>154<br>4.7.1二阶转移概率张量的估计155<br>4.7.2二阶隐马尔可夫模型中的维特比算法156<br>4.7.3二阶隐马尔可夫模型应用于中文分词158<br>4.8总结..159<br>第5章感知机分类与序列标注.160<br>5.1分类问题160<br>5.1.1定义160<br>5.1.2应用161<br>5.2线性分类模型与感知机算法161<br>5.2.1特征向量与样本空间.162<br>5.2.2决策边界与分离超平面164<br>5.2.3感知机算法..167<br>5.2.4损失函数与随机梯度下降<em>169<br>5.2.5投票感知机和平均感知机171<br>5.3基于感知机的人名性别分类174<br>5.3.1人名性别语料库.174<br>5.3.2特征提取174<br>5.3.3训练175<br>5.3.4预测176<br>5.3.5评测177<br>5.3.6模型调优178<br>5.4结构化预测问题..180<br>5.4.1定义180<br>5.4.2结构化预测与学习的流程180<br>5.5线性模型的结构化感知机算法..180<br>5.5.1结构化感知机算法.180<br>5.5.2结构化感知机与序列标注182<br>5.5.3结构化感知机的维特比解码算法..183<br>5.6基于结构化感知机的中文分词..186<br>5.6.1特征提取187<br>5.6.2多线程训练..189<br>5.6.3特征裁剪与模型压缩</em>.190<br>5.6.4创建感知机分词器.192<br>5.6.5准确率与性能..194<br>5.6.6模型调整与在线学习<em>.195<br>5.6.7中文分词特征工程</em>.197<br>5.7总结..199<br>第6章条件随机场与序列标注.200<br>6.1机器学习的模型谱系200<br>6.1.1生成式模型与判别式模型201<br>6.1.2有向与无向概率图模型202<br>6.2条件随机场..205<br>6.2.1线性链条件随机场.205<br>6.2.2条件随机场的训练<em>207<br>6.2.3对比结构化感知机.210<br>6.3条件随机场工具包.212<br>6.3.1CRF++的安装212<br>6.3.2CRF++语料格式213<br>6.3.3CRF++特征模板214<br>6.3.4CRF++命令行训练215<br>6.3.5CRF++模型格式</em>216<br>6.3.6CRF++命令行预测217<br>6.3.7CRF++代码分析*218<br>6.4HanLP中的CRF++API220<br>6.4.1训练分词器..220<br>6.4.2标准化评测..220<br>6.5总结..221<br>第7章词性标注.222<br>7.1词性标注概述.222<br>7.1.1什么是词性..222<br>7.1.2词性的用处..223<br>7.1.3词性标注223<br>7.1.4词性标注模型..223<br>7.2词性标注语料库与标注集.224<br>7.2.1《人民日报》语料库与PKU标注集..225<br>7.2.2国家语委语料库与863标注集.231<br>7.2.3《诛仙》语料库与CTB标注集..234<br>7.3序列标注模型应用于词性标注..236<br>7.3.1基于隐马尔可夫模型的词性标注..237<br>7.3.2基于感知机的词性标注238<br>7.3.3基于条件随机场的词性标注..240<br>7.3.4词性标注评测..241<br>7.4自定义词性..242<br>7.4.1朴素实现242<br>7.4.2标注语料243<br>7.5总结..244<br>第8章命名实体识别.245<br>8.1概述..245<br>8.2基于规则的命名实体识别.246<br>8.3命名实体识别语料库..250<br>8.4基于层叠隐马尔可夫模型的角色标注框架252<br>8.5基于序列标注的命名实体识别..260<br>8.6自定义领域命名实体识别.266<br>8.7总结..268<br>第9章信息抽取.270<br>9.1新词提取270<br>9.2关键词提取..276<br>9.3短语提取283<br>9.4关键句提取..284<br>9.5总结..287<br>第10章文本聚类.288<br>10.1概述..288<br>10.2文档的特征提取291<br>10.3k均值算法293<br>10.4重复二分聚类算法..300<br>10.5标准化评测..303<br>10.6总结..305<br>第11章文本分类.306<br>11.1文本分类的概念306<br>11.2文本分类语料库307<br>11.3文本分类的特征提取.308<br>11.4朴素贝叶斯分类器..312<br>11.5支持向量机分类器..317<br>11.6标准化评测..320<br>11.7情感分析321<br>11.8总结..323<br>第12章依存句法分析.324<br>12.1短语结构树..324<br>12.1.3宾州树库和中文树库.326<br>12.2依存句法树..327<br>12.3依存句法分析.333<br>12.4基于转移的依存句法分析..334<br>12.5依存句法分析API340<br>12.6案例：基于依存句法树的意见抽取..342<br>12.7总结..344<br>第13章深度学习与自然语言处理345<br>13.1传统方法的局限345<br>13.2深度学习与优势348<br>13.3word2vec..353<br>13.4基于神经网络的高性能依存句法分析器.360<br>13.5自然语言处理进阶..363<br>自然语言处理学习资料推荐..365</p>
<h2 id="近似算法的设计与分析"><a href="#近似算法的设计与分析" class="headerlink" title="近似算法的设计与分析"></a>近似算法的设计与分析</h2><p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/6782289/">https://book.douban.com/subject/6782289/</a></p>
<p>作者: <a target="_blank" rel="noopener" href="https://book.douban.com/search/堵丁柱">堵丁柱</a> / <a target="_blank" rel="noopener" href="https://book.douban.com/search/葛可一">葛可一</a> / <a target="_blank" rel="noopener" href="https://book.douban.com/search/胡晓东">胡晓东</a></p>
<img src="/2021/02/Reading-list-202102/image-20210220031230930.png" class="" title="image-20210220031230930">
<h3 id="内容简介-4"><a href="#内容简介-4" class="headerlink" title="内容简介"></a>内容简介</h3><p>《近似算法的设计与分析》分为五个部分：首先，在第一部分，即第一章，我们简明扼要地介绍NP—完全性和近似算法的概念。在第二部分，也就是第二章，我们对贪婪算法进行深人的分析，包括以次模函数为势函数的贪婪算法和以非次模函数为势函数的贪婪算法。第三部分包含三章：第三章、第四章和第五章。在这三章中我们讨论多种限制方法，其中包含用于处理几何问题的划分和断切方法。第四部分包含第六章、第七章、第八章和第九章。在这四章中我们主要讨论松弛方法。在第六章中我们对松弛方法进行一般性的讨论以后，在紧接着的三章中，讨论基于线性和半定规划的近似算法设计，包括原始对偶方案和与之等价的局部比值方法。在最后一部分，即第十章，我们介绍应用NP—完全性理论的近期成果所取得的各种不可近似性结果。</p>
<h3 id="目录-4"><a href="#目录-4" class="headerlink" title="目录"></a>目录</h3><p>《近似算法的设计与分析》<br>第一章 引言<br>1.1 “芝麻，开门!”<br>1.2 近似算法的设计技巧<br>1.3 启发式算法与近似算法<br>1.4 计算复杂性的术语<br>1.5 np-完全问题<br>1.6 性能比<br>习题<br>历史注记<br>第二章 贪婪策略<br>2.1 独立系统<br>2.2 拟阵<br>2.3 权函数的四边形条件<br>2.4 次模势函数<br>2.5 应用<br>2.6 非次模势函数<br>习题<br>历史注记<br>第三章 限制<br>.3.1 斯坦纳树和生成树<br>3.2 k-限制斯坦纳树<br>3.3 贪婪k-限制斯坦纳树<br>3.4 最小生成树的应用<br>3.5 种系进化树同步<br>习题<br>历史注记<br>第四章 划分<br>4.1 划分与移位<br>4.2 边界区域<br>4.3 多层划分<br>4.4 双重划分<br>4.5 树划分<br>习题<br>历史注记<br>第五章 断切<br>5.1 矩形划分<br>5.2 l-断切<br>5.3 m-断切<br>5.4 接口<br>5.5 四叉树划分与补缀<br>5.6 两阶段接口<br>习题<br>历史注记<br>第六章 松弛<br>6.1 有向哈密顿圈和超串<br>6.2 两阶段贪婪近似算法<br>6.3 单位圆盘图上连通控制集<br>6.4 有向图中的强连通控制集<br>6.5 光纤网络中的多播路由<br>6.6 关于松弛与限制的附记<br>习题<br>历史注记<br>第七章 线性规划<br>7.1 基本性质<br>7.2 单纯形法<br>7.3 组合舍人<br>7.4 管输舍人<br>7.5 迭代舍人<br>7.6 随机舍人<br>习题<br>历史注记<br>第八章 原始对偶方案与局部比值法<br>8.1 对偶理论和原始对偶方案<br>8.2 广义覆盖<br>8.3 网络设计<br>8.4 局部比值法<br>8.5 再论等价性<br>习题<br>历史注记<br>第九章 半定规划<br>9.1 谱面体<br>9.2 半定规划<br>9.3 超平面舍人<br>9.4 旋转向量<br>9.5 多元正交舍人<br>习题<br>历史注记<br>第十章 不可近似性<br>10.1 具有间隙的多一归约<br>10.2 间隙放大与保持<br>10.3 apx-完全性<br>10.4 概率可验证明定理<br>10.5 (ρin n)-不可近似性<br>10.6 nc-不可近似性<br>习题<br>历史注记<br>参考文献<br>名词索引(汉英对照)</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/02/%E4%BB%8E%E6%9C%AA%E7%9C%9F%E6%AD%A3%E6%98%8E%E7%99%BD%E8%BF%87%E7%9A%84%E5%B7%A6%E5%92%8C%E5%8F%B3/" rel="prev" title="从未真正明白过的左和右">
      <i class="fa fa-chevron-left"></i> 从未真正明白过的左和右
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/03/%E4%B8%80%E6%96%87%E5%85%A5%E9%97%A8Bert/" rel="next" title="一文入门Bert">
      一文入门Bert <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%AD%A3%E7%BB%8F%E4%B9%A6%E9%98%85%E8%AF%BB"><span class="nav-number">1.</span> <span class="nav-text">正经书阅读</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1-%E5%9C%A8%E8%AF%BB"><span class="nav-number">1.1.</span> <span class="nav-text">概率论与数理统计[在读]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AE%B9%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.1.</span> <span class="nav-text">内容简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95"><span class="nav-number">1.1.2.</span> <span class="nav-text">目录</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8-%E5%9C%A8%E8%AF%BB"><span class="nav-number">1.2.</span> <span class="nav-text">自然语言处理入门[在读]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AE%B9%E7%AE%80%E4%BB%8B-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">内容简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%9C%E8%80%85%E7%AE%80%E4%BB%8B"><span class="nav-number">1.2.2.</span> <span class="nav-text">作者简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95-1"><span class="nav-number">1.2.3.</span> <span class="nav-text">目录</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%83%B3%E8%AF%BB"><span class="nav-number">1.3.</span> <span class="nav-text">神经网络与深度学习[想读]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AE%B9%E7%AE%80%E4%BB%8B-2"><span class="nav-number">1.3.1.</span> <span class="nav-text">内容简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%9C%E8%80%85%E7%AE%80%E4%BB%8B-1"><span class="nav-number">1.3.2.</span> <span class="nav-text">作者简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95-2"><span class="nav-number">1.3.3.</span> <span class="nav-text">目录</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%97%B2%E4%B9%A6%E9%98%85%E8%AF%BB"><span class="nav-number">2.</span> <span class="nav-text">闲书阅读</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BF%83%E6%B5%81-%E5%AE%8C"><span class="nav-number">2.1.</span> <span class="nav-text">心流[完]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%86%E5%8F%B2%E6%B7%B1%E5%A4%84%E7%9A%84%E5%BF%A7%E8%99%91-%E5%AE%8C"><span class="nav-number">2.2.</span> <span class="nav-text">历史深处的忧虑[完]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A3%B0%E9%9F%B3%E7%9A%84%E9%AD%85%E5%8A%9B-%E5%AE%8C"><span class="nav-number">2.3.</span> <span class="nav-text">声音的魅力[完]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%E7%AE%80%E5%8F%B2-%E5%9C%A8%E8%AF%BB"><span class="nav-number">2.4.</span> <span class="nav-text">中国哲学简史[在读]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%81%93%E5%BE%B7%E7%BB%8F%E4%B8%8B%E7%AF%87-%E5%9C%A8%E8%AF%BB"><span class="nav-number">2.5.</span> <span class="nav-text">道德经下篇[在读]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%9D%A1%E8%A7%89-%E5%9C%A8%E8%AF%BB"><span class="nav-number">2.6.</span> <span class="nav-text">为什么要睡觉[在读]</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%AD%A3%E7%BB%8F%E4%B9%A6%E8%B4%AD%E5%85%A5"><span class="nav-number">3.</span> <span class="nav-text">正经书购入</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8-%E5%B7%B2%E8%B4%AD"><span class="nav-number">3.1.</span> <span class="nav-text">自然语言处理入门[已购]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AE%B9%E7%AE%80%E4%BB%8B-3"><span class="nav-number">3.1.1.</span> <span class="nav-text">内容简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%9C%E8%80%85%E7%AE%80%E4%BB%8B-2"><span class="nav-number">3.1.2.</span> <span class="nav-text">作者简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95-3"><span class="nav-number">3.1.3.</span> <span class="nav-text">目录</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90"><span class="nav-number">3.2.</span> <span class="nav-text">近似算法的设计与分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AE%B9%E7%AE%80%E4%BB%8B-4"><span class="nav-number">3.2.1.</span> <span class="nav-text">内容简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95-4"><span class="nav-number">3.2.2.</span> <span class="nav-text">目录</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="匠心圆头"
      src="/images/zsy.jpg">
  <p class="site-author-name" itemprop="name">匠心圆头</p>
  <div class="site-description" itemprop="description">致虚极，守静笃。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">126</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/shira0905" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;shira0905" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:shira0905@gmail.com" title="E-Mail → mailto:shira0905@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021-07-11</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">匠心圆头</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
